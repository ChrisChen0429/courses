4.5/0.125
7.875/0.125
qf(p=0.95,1,14)
qf(36,1,14,lower.tail = FALSE)
qf(36,1,14,lower.tail = FALSE)
pf(36,1,14,lower.tail = FALSE)
qf(p=0.95,2,14)
pf(63,2,14,lower.tail = FALSE)
knitr::opts_chunk$set(echo = TRUE)
response <- c(19.4,20.6,20,25,24.5,24,24.8,26,25.4,23.1,24.3,23.7,22.6,21.6,22.1,25.6,26.8,26.2,27.6,26.4,27,25.4,24.5,26.3)
wheat <- c(rep(rep("M",3),rep("N",3),rep("O",3),rep("P",3),2))
fertilizer <- c(rep("A",12),rep("B",12))
wheat <- c(rep(c(rep("M",3),rep("N",3),rep("O",3),rep("P",3)),2))
boxplot(response~fertilizer*wheat)
interaction.plot(fertilizer,wheat,response)
length(response)
summary(aov(response~fertilizer*wheat))
summary(aov(response~fertilizer+wheat))
fit<-aov(response~fertilizer)
tk<-TukeyHSD(fit, "A")
tk<-TukeyHSD(fit, "fertilizer")
fit<-aov(response~fertilizer)
tk<-TukeyHSD(fit, "fertilizer")
plot(tk)
tk
fit<-aov(response~fertilizer)
tk<-TukeyHSD(fit)
plot(tk)
tk
fit1<-aov(response~fertilizer)
tk2<-TukeyHSD(fit1)
plot(tk1)
fit1<-aov(response~fertilizer)
tk1<-TukeyHSD(fit1)
plot(tk1)
tk1
fit2<-aov(response~fertilizer)
tk2<-TukeyHSD(fit2)
plot(tk2)
tk2
fit2<-aov(response~wheat)
tk2<-TukeyHSD(fit2)
plot(tk2)
tk2
qf(0.05,2,20)
qf(0.95,2,20)
qf(0.95,2,27)
4.5-2.3
4.5-3
1.5/2.2
1-0.6818182
knitr::opts_chunk$set(echo = TRUE)
a = (17 - 0.34 * sqrt(66))
a
b = (0.34 * sqrt(66) - 22)
c = 11 - 5.5
root1 <-  (-1 * b + sqrt(b^2 - 4 * a * c))/(2*a)
root2 <-  (-1 * b - sqrt(b^2 - 4 * a * c))/(2*a)
root1
root2
1-0.9403999
1-0.4107773
0.9403999 * 0.023 + 0.0596001 * 0.045
0.4107773 * 0.023 + 0.5892227 * 0.045
5/7
0.7142857 * 0.65
0.7142857 * 0.35
1-0.7142857
75*300/(75*300+115*100)
1-0.6617647
(100*200)/(100*200+125*100)
(125*100)/(100*200+125*100)
0.6153846*0.001+0.3846154*0.0015
0.6153846^2*0.001^2+0.3846154^2*0.0015^2+2*0.6153846*0.3846154*0.35*0.03*0.04
sqrt(0.0001995281)
8/13
(8/13)^2*(0.003)^2*+(5/13)^2*(0.004)^2+2*(8/13)*(5/13)*0.35*0.003*0.004
0.0001995281
sqrt(1.988174e-06)
(8/13)^2*(0.03)^2*+(5/13)^2*(0.04)^2+2*(8/13)*(5/13)*0.35*0.03*0.04
(8/13)^2*(0.03)^2+(5/13)^2*(0.04)^2+2*(8/13)*(5/13)*0.35*0.03*0.04
sqrt(0.0007763314)
0.45^2
0.58^3
0.63^10
curve(expr = exp(-8*abs(x)^2),(-1,1))
curve(expr = exp(-8*abs(x)^2),from = -1,to = 1))
curve(expr = exp(-8*abs(x)^2),from = -1,to = 1)
pf(q = 1.73,2,3)
pf(q = 1.73,2,3,lower.tail = FALSE)
qf(P=0.3164704,2,3,lower.tail = FALSE)
qf(P=0.3164704,2,3)
qf(p = 0.5,2,3)
lda.pred = predict(lda.model, iris[-train,])
return
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/cheny/Desktop/study/second term/Statistical Method In Finance/homework/homework three")
data <- read.csv("hw3.csv")
library(MASS)
library(quadprog)
price <- cbind(data$CAT_AC,data$IBM_AC,data$MSFT_AC)
n <- dim(price)[1]
return <- log(price[2:n,]/price[1:(n-1),])
mu <- colMeans(return)
sigma <- cov(return)
return
a = return[,c(1,2)]
a
cor(a)
data
a = data[,c(2,3)]
a
cor(a)
var(a)
252/(252+81)
252 + 23
235 + 138
exp(0.1 + 0.25^2/2)
sqrt(exp(0.1+0.25^2/2)*(exp(0.25^2)-1))
qt(p=1-0.05/2)
qt(p=1-0.05/2,df=3)
9.125 + 3.095696 * 3.182446
9.125 + 0.3095696 * 3.182446
9.125 - 0.3095696 * 3.182446
knitr::opts_chunk$set(echo = TRUE)
data2 <- read.table("Hayfever.txt",header = TRUE)
data2$A <- as.factor(data2$A)
data2$B <- as.factor(data2$B)
# interaction plot
interaction.plot(data2$A,data2$B,data2$Hoursofrel.)
summary(aov(data2$Hoursofrel. ~ data2$A*data2$B))
summary(aov(data2$Hoursofrel. ~ data2$A:data2$B))
fit <- aov(data2$Hoursofrel. ~ data2$A:data2$B)
TukeyHSD(fit)
aov(data2$Hoursofrel. ~ data2$A:data2$B)
aonva(data2$Hoursofrel. ~ data2$A:data2$B)
anova(data2$Hoursofrel. ~ data2$A:data2$B)
aov(data2$Hoursofrel. ~ data2$A:data2$B)
lm(data2$Hoursofrel. ~ data2$A * data2$B)
lm(data2$Hoursofrel. ~ data2$A * data2$B - data2$A - data2$B)
summary(lm(data2$Hoursofrel. ~ data2$A * data2$B - data2$A - data2$B))
summary(lm(data2$Hoursofrel. ~ data2$A * data2$B - data2$A - data2$B ))
lm(data2$Hoursofrel. ~ data2$A * data2$B - data2$A - data2$B
lm(data2$Hoursofrel. ~ data2$A * data2$B - data2$A - data2$B
lm(data2$Hoursofrel. ~ data2$A * data2$B - data2$A - data2$B)
lm(data2$Hoursofrel. ~ data2$A * data2$B - data2$A - data2$B - 1)
summary(lm(data2$Hoursofrel. ~ data2$A * data2$B - data2$A - data2$B - 1))
data2$Hoursofrel.[(data2$A==1 & data2$B == 1),]
data2$Hoursofrel.[(data2$A==1 & data2$B == 1)]
mean(data2$Hoursofrel.[(data2$A==1 & data2$B == 1)])
data
# read the data
setwd("C:/Users/cheny/Desktop/study/second term/Advanced Data Analysis/homework/homework three")
data <- read.table("sofdrink.txt",header = TRUE)
fit <- lm(data$Time_lapse ~ data$Agent)
summary(fit)
fit <- lm(data$Time_lapse ~ data$Agent-1)
summary(fit)
data$Agent <- as.factor(data$Agent)
fit <- lm(data$Time_lapse ~ data$Agent-1)
summary(fit)
predict.lm(fit)
predict(fit)
predict(fit,newdata = c(0.25,0.2,0.2,0.2,0.15))
data$Agent <- as.factor(data$Agent)
fit <- lm(data$Time_lapse ~ data$Agent-1)
summary(fit)
predict(fit,newdata = c(0.25,0.2,0.2,0.2,0.15))
predict(fit,newdata = c(1),interval = 'confidence')
summary(fit)
predict(fit,newdata = c(1,1,1,1,1),interval = 'confidence')
predict(fit,newdata = c(0.25),interval = 'confidence')
summary(fit)
0.25 * 24.5500 + 0.2 * 22.5500 + 0.2*11.7500 +0.2* 14.8000 +0.15*30.1000
0.6133^2*(0.25^2 + 0.2^2 * 3 + 0.15^2)
sqrt(0.6133^2*(0.25^2 + 0.2^2 * 3 + 0.15^2))
View(data)
qt(p=1-0.05/2 , df=30)
qt(p=1-0.05/2 , df=30) * sqrt(1/2) * 1.157507
2.125 + 1.671561
2.125 - 1.671561
t.test(data2$Hoursofrel.[data2$A==2 & data2$B == 3])
t.test(data2$Hoursofrel.[data2$A==2 & data2$B == 3])
setwd("C:/Users/cheny/Desktop/study/second term/staistical machine learning/homework/homework two")
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/cheny/Desktop/study/second term/staistical machine learning/homework/homework two")
library(datasets)
statedata=as.data.frame(state.x77)
colnames(statedata)=c("popu", "inc", "illit", "life.exp", "murder", "hs.grad", "frost", "area")
plot(life.exp~inc, data=statedata)
cor(statedata[,"life.exp"], statedata[,"inc"])
plot(life.exp~inc, data=statedata, type="n")
text(life.exp~inc, data=statedata, state.abb)
model1=lm(life.exp~inc, data=statedata)
summary(model1)
plot(life.exp~inc, data=statedata,
xlab="Life Expectancy", ylab="Income")
abline(model1)
par(mfrow=c(2,2)) # create a panel of four plotting areas
for(i in 1:4){
## Plot the population
plot(life.exp~inc, data=statedata,
xlab="Life Expectancy", ylab="Income",
title=paste("Random sample", format(i)),
ylim=c(min(life.exp), max(life.exp)+0.3))
abline(model1)
if(i==1){
legend(3030, 74.2,
pch=c(NA, NA, NA, 1, 16),
lty=c(1, 1, 2, NA, NA),
col=c(1, 2, 2, 1, 2),
c("population truth", "sample estimate",
"sample confidence band",
"states", "sampled"),
cex=0.7,
bty="n"
)
}
## Select the sample
selected.states=sample(1:50, 10)
points(statedata[selected.states,"inc"],
statedata[selected.states,"life.exp"], pch=16, col=2)
## Fit a regression line using the sample
model.sel = lm(life.exp~inc, data=statedata[selected.states,])
abline(model.sel, col=2)
## Make a confidence band.
#### first calculate the width of the band, W.
ww=qt(0.975, 10-2)
#### generate plotting X values.
plot.x<-data.frame(inc=seq(3000, 7000, 1))
#### se.fit=T is an option to save
#### the standard error of the fitted values.
plot.fit<-predict(model.sel, plot.x,
level=0.95, interval="confidence",
se.fit=T)
#### lines is a function to add connected lines
#### to an existing plot.
lines(plot.x$inc, plot.fit$fit[,1]+ww*plot.fit$se.fit,
col=2, lty=2)
lines(plot.x$inc, plot.fit$fit[,1]-ww*plot.fit$se.fit,
col=2, lty=2)
}
library(MASS)
library(ISLR)
attach(Boston)
lm.fit=lm(medv ~ lstat+age,data=Boston)
summary(lm.fit)
lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)
lm.fit1=lm(medv ~.-age,data=Boston)
summary(lm.fit1)
lm.fit1=lm(medv ~ .-age,data=Boston)
summary(lm.fit1)
summary(lm(medv ~ lstat*age,data=Boston))
plot(lstat, medv, pch=16)
lm.fit2=lm(medv ~ lstat+I(lstat^2))
summary(lm.fit2)
lm.fit=lm(medv~lstat)
anova(lm.fit ,lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
names(Carseats)
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
attach(Carseats)
contrasts(ShelveLoc)
iris = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data", sep = ",", header = FALSE)
names(iris) = c("sepal.length", "sepal.width", "petal.length", "petal.width", "iris.type")
### attach name to each column so that we can directly access each column by its name
attach(iris)
train = sample.int(nrow(iris), 100)
Y = iris.type == "Iris-setosa"
logistic.model = glm(Y ~ sepal.length + sepal.width, data=iris, family = binomial(), subset=train)
logistic.model
plot(sepal.length[train], sepal.width[train], type='p',pch=16, col=(Y[train]+4), xlab="Sepal Length", ylab="Sepal Width")
abline(a = -logistic.model$coefficients[1]/logistic.model$coefficients[3], b = -logistic.model$coefficients[2]/logistic.model$coefficients[3], col='gray', lwd=2)
glm.probs = predict(logistic.model, iris[-train,], type="response")
glm.pred = glm.probs>0.5
### summrize the prediction by a confusion matrix
table(Y[-train], glm.pred)
library(MASS)
lda.model<-lda(Y ~ sepal.length + sepal.width, data=iris, subset=train)
lda.model
plot(lda.model)
lda.pred = predict(lda.model, iris[-train,])
table(Y[-train], lda.pred$class)
knitr::opts_chunk$set(echo = TRUE)
train_3 <- read.table("train_3.txt")
setwd("C:/Users/cheny/Desktop/study/second term/staistical machine learning/homework/homework two")
train_3 <- read.table("train_3.txt")
View(train_3)
setwd("C:/Users/cheny/Desktop/study/second term/staistical machine learning/homework/homework two")
train_3 <- read.table("train_3.txt")
train_5 <- read.table("train_5.txt")
train_8 <- read.table("train_8.txt")
setwd("C:/Users/cheny/Desktop/study/second term/staistical machine learning/homework/homework two")
train_3 <- read.table("train_3.txt", header=FALSE, sep=",")
train_5 <- read.table("train_5.txt", header=FALSE, sep=",")
train_8 <- read.table("train_8.txt", header=FALSE, sep=",")
# check the data
dim(train_3)
setwd("C:/Users/cheny/Desktop/study/second term/staistical machine learning/homework/homework two")
train_3 <- read.table("train_3.txt", header=FALSE, sep=",")
train_5 <- read.table("train_5.txt", header=FALSE, sep=",")
train_8 <- read.table("train_8.txt", header=FALSE, sep=",")
# check the data
dim(train_3)
dim(train_5)
dim(train_8)
library(MASS)
View(train_3)
library(glmnet)
download.packages("glmnet")
download.packages(glmnet)
install.packages("glmnet")
library(glmnet)
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/cheny/Desktop/study/second term/staistical machine learning/homework/homework two")
train_3 <- read.table("train_3.txt", header=FALSE, sep=",")
train_3$number <- 3
train_5 <- read.table("train_5.txt", header=FALSE, sep=",")
train_5$number <- 5
train_8 <- read.table("train_8.txt", header=FALSE, sep=",")
train_8$number <- 8
train_data <- rbind(train_3,train_5,train_8)
test <- read.table("zip_test.txt",header = FALSE,sep = " ")
number <- test[,1]
test_data <- test[,-1]
test_data$number <- number
colnames(test_data) <- c("V1",colnames(test_data)[-256])
# we only need the testing data which is represent the number 3,5,8
test_data <- test_data[(test_data$number==3 | test_data$number==5|test_data$number==8),]
library(MASS)
lda.model<-lda(number ~.-number , data=train_data)
lda.pred1 = predict(lda.model, train_data[,-257])
table(train_data$number, lda.pred1$class)
lda.pred2 = predict(lda.model, test_data[,-257])
table(test_data$number, lda.pred2$class)
library(psych)
x_data <- train_data[,-257]
x_data <- scale(x_data)
pc <- principal(x_data,nfactors = 49,scores = T,rotate = "varimax")
score <- as.data.frame(pc$scores)
colnames(score) <- colnames(train_data)[1:49]
score$number <- train_data$number
lda.model2<-lda(number ~.-number , data=score)
lda.pred3 = predict(lda.model2, score[,-50])
table(score$number, lda.pred3$class)
lda.pred4 = predict(lda.model2, test_data[,-257])
table(test_data$number, lda.pred4$class)
# rebulit training 3
a <- train_3[seq(1,nrow(train_3),2),]
b <- train_3[seq(2,nrow(train_3),2),]
c <- a+b
e <- c[,seq(1,256,2)]
f <- c[,seq(2,256,2)]
new_train_3 <- e+f
new_train_3$number = 3
# rebulit training 5
a <- train_5[seq(1,nrow(train_5),2),]
b <- train_5[seq(2,nrow(train_5),2),]
c <- a+b
e <- c[,seq(1,256,2)]
f <- c[,seq(2,256,2)]
new_train_5 <- e+f
new_train_5$number = 5
# rebulit training 8
a <- train_8[seq(1,nrow(train_8),2),]
b <- train_8[seq(2,nrow(train_8),2),]
c <- a+b
e <- c[,seq(1,256,2)]
f <- c[,seq(2,256,2)]
new_train_8 <- e+f
new_train_8$number = 8
new_train <- rbind(new_train_3,new_train_5,new_train_8)
# rebulit the testing data
test_3 <- test_data[test_data$number==3,]
a <- test_3[seq(1,nrow(test_3),2),]
b <- test_3[seq(2,nrow(test_3),2),]
c <- a+b
e <- c[,seq(1,256,2)]
f <- c[,seq(2,256,2)]
new_test_3 <- e+f
new_test_3$number = 3
# rebulit test 5
test_5 <- test_data[test_data$number==5,]
a <- test_5[seq(1,nrow(test_5),2),]
b <- test_5[seq(2,nrow(test_5),2),]
c <- a+b
e <- c[,seq(1,256,2)]
f <- c[,seq(2,256,2)]
new_test_5 <- e+f
new_test_5$number = 5
# rebulit test 8
test_8 <- test_data[test_data$number==8,]
a <- test_8[seq(1,nrow(test_8),2),]
b <- test_8[seq(2,nrow(test_8),2),]
c <- a+b
e <- c[,seq(1,256,2)]
f <- c[,seq(2,256,2)]
new_test_8 <- e+f
new_test_8$number = 8
new_test <- rbind(new_test_3,new_test_5,new_test_8)
lda.model3<-lda(number ~.-number , data=new_train)
lda.pred5 = predict(lda.model3, new_train[,-129])
table(new_train$number, lda.pred5$class)
lda.pred6 = predict(lda.model3, new_test[,-129])
table(new_test$number, lda.pred6$class)
library(glmnet)
glm.fit=glmnet(as.matrix(new_train[,-129]),y = as.factor(new_train$number), family = "multinomial")
pred = predict(object = glm.fit, newx=as.matrix(new_train[,-129]),type="class")
table(new_train$number, pred[,89])
table(new_train$number, pred[,1])
table(new_train$number, pred[,10])
table(new_train$number, pred[,89])
x_data <- test_data[,-257]
x_data <- scale(x_test)
x_data <- test_data[,-257]
x_data <- scale(x_data)
pc <- principal(x_data,nfactors = 49,scores = T,rotate = "varimax")
score <- as.data.frame(pc$scores)
colnames(score) <- colnames(test_data)[1:49]
score$number <- test_data$number
lda.pred4 = predict(lda.model2, test_data[,-257])
table(test_data$number, lda.pred4$class)
x_data <- test_data[,-257]
x_data <- scale(x_data)
pc <- principal(x_data,nfactors = 49,scores = T,rotate = "varimax")
score2 <- as.data.frame(pc$scores)
colnames(score2) <- colnames(test_data)[1:49]
score2$number <- test_data$number
lda.pred4 = predict(lda.model2, score2[,-50])
table(score2$number, lda.pred4$class)
x_data <- test_data[,-257]
x_data <- scale(x_data)
pc <- principal(x_data,nfactors = 49,scores = T,rotate = "varimax")
score2 <- as.data.frame(pc$scores)
colnames(score2) <- colnames(test_data)[1:49]
score2$number <- test_data$number
lda.model2<-lda(number ~.-number , data=score2)
lda.pred4 = predict(lda.model2, score2[,-50])
table(score2$number, lda.pred4$class)
8 + 4 + 7 + 2 + 7 + 4
32/492
setwd("C:/Users/cheny/Desktop/study/second term/staistical machine learning/homework/homework two")
#############################
#Problem 2      #############
#code from Prof' Liu ########
############################
Credit<-read.csv("Credit.csv", header=T)
############################
Credit<-read.csv("Credit.csv", header=T)
Credit<-data.frame(Credit[,-1])
colnorm<-apply(Credit[,c(1:6,11)],2,var)
Credit[,c(1:6,11)]<- as.matrix(Credit[,c(1:6,11)]) %*% diag(1/sqrt(colnorm))
library(leaps)
### Best subsect selection ###
regfit.full=regsubsets(Balance~., Credit, nvmax=20, method="exhaustive")
regfull.summary<-summary(regfit.full)
regfull.summary$rss
pdf("best.pdf",height=3,width=6)
par(oma=c(1,1,1,1),pty='s', mar=c(2,2,1,1),mgp=c(1.5,0.25,0),
lwd=0.5,tck=-0.01, cex.axis=0.5, cex.lab=0.9, cex.main=0.9,pty='s')
plot(c(1:11), regfull.summary$rss,,type='p',pch=16, col="red", xlab="", ylab=""
,axes=F)
lines(c(1:11),regfull.summary$rss,col=2)
axis(1,lwd=0.5)
axis(2,lwd=0.5)
box(lwd=par()$lwd)
mtext(1,text="Subset Size k",cex=0.6,line = 1)
mtext(2,text="Residual Sum-of-Squares",line = 1,cex=0.6)
### Forward stepwise selection ###
regfit.fwd=regsubsets(Balance~., data=Credit, nvmax=20, method="forward")
regfwd.summary<-summary(regfit.fwd)
points(c(1:11)+0.05, regfwd.summary$rss , pch=18, col="blue")
lines(c(1:11)+0.05,regfwd.summary$rss,col="blue")
### Backward stepwise selection ###
regfit.bwd=regsubsets(Balance~., data=Credit, nvmax=20, method="backward")
regbwd.summary<-summary(regfit.bwd)
points(c(1:11)+0.1, regbwd.summary$rss, pch=8, col="green")
lines(c(1:11)+0.1,regbwd.summary$rss,col="green")
legend(x="topright",legend=c("Best subsect selection", "Forward stepwise selection", "Backward stepwise selection"),col=c(2,"blue","green"),
pch=c(16,18,8),cex=0.5,border = NA,box.lty=0)
dev.off()
which.min(regfull.summary$bic)
which.min(regfull.summary$cp)
which.min(regfwd.summary$bic)
which.min(regfwd.summary$cp)
which.min(regbwd.summary$bic)
###############
#Problem 3#####
###############
d3=read.table("train.3.txt",header = F,sep=',')
d5=read.table("train.5.txt",header = F,sep=',')
library(MASS)
library(glmnet)
d8=read.table("train.8.txt",header = F,sep=',')
test=read.table("ziptest.txt",header = F)
names(test)=c("y",names(test))[1:257]
test=test[test$y%in%c(3,5,8),]
acc=matrix(NA,4,2)
d3$y=3
d5$y=5
d8$y=8
d=rbind(d3,d5,d8)
lad.full=lda(y~.,data=d)
acc[1,1]=mean(predict(lad.full,d)$class == d$y)
acc[1,2]=mean(predict(lad.full,test)$class == test$y)
prin_comp=prcomp(d[1:256], scale. = T)
std_dev = prin_comp$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
