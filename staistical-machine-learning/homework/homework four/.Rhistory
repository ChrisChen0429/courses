new_train_3$number = 3
# rebulit training 5
a <- train_5[seq(1,nrow(train_5),2),]
b <- train_5[seq(2,nrow(train_5),2),]
c <- a+b
e <- c[,seq(1,256,2)]
f <- c[,seq(2,256,2)]
new_train_5 <- e+f
new_train_5$number = 5
# rebulit training 8
a <- train_8[seq(1,nrow(train_8),2),]
b <- train_8[seq(2,nrow(train_8),2),]
c <- a+b
e <- c[,seq(1,256,2)]
f <- c[,seq(2,256,2)]
new_train_8 <- e+f
new_train_8$number = 8
new_train <- rbind(new_train_3,new_train_5,new_train_8)
lda.model3<-lda(number ~.-number , data=new_train)
lda.pred5 = predict(lda.model3, new_train[,-129])
table(new_train$number, lda.pred5$class)
lda.pred6 = predict(lda.model3, test_data[,-257])
table(test_data$number, lda.pred6$class)
337/492
a <- matrix(c(1,0.5,0,0.5,1,0,0,0,1),nrow = 3)
a
svd(a)
knitr::opts_chunk$set(echo = TRUE)
library(psych)
principal(a)
b <- principal(a)
b$loadings
svd(a)
princomp(a)
b <- princomp(a)
b$loadings
a <- matrix(c(1,0.7,0,0.7,1,0,0,0,1),nrow = 3)
svd(a)
knitr::opts_chunk$set(echo = TRUE)
#input the data
social_economics <- c(rep('low',4),rep('Median',4),rep('high',4))
Boy_Scout <- c(rep(c(rep('Yes',2),ep('No',2)),3))
Boy_Scout <- c(rep(c(rep('Yes',2),rep('No',2)),3))
deliquency <- c(rep(c('Yes','No'),6))
fequency <- c(10,40,40,160,18,132,18,132,8,192,2,48)
data <- rbind(social_economics,Boy_Scout,deliquency)
#input the data
social_economics <- c(rep('low',4),rep('Median',4),rep('high',4))
Boy_Scout <- c(rep(c(rep('Yes',2),rep('No',2)),3))
deliquency <- c(rep(c('Yes','No'),6))
fequency <- c(10,40,40,160,18,132,18,132,8,192,2,48)
data <- rbind(social_economics,Boy_Scout,deliquency)
View(data)
#input the data
social_economics <- c(rep('low',4),rep('Median',4),rep('high',4))
Boy_Scout <- c(rep(c(rep('Yes',2),rep('No',2)),3))
deliquency <- c(rep(c('Yes','No'),6))
frequency <- c(10,40,40,160,18,132,18,132,8,192,2,48)
data <- cbind(social_economics,Boy_Scout,deliquency,frequency)
View(data)
#input the data
social_economics <- c(rep('low',4),rep('Median',4),rep('high',4))
Boy_Scout <- c(rep(c(rep('Yes',2),rep('No',2)),3))
deliquency <- c(rep(c('Yes','No'),6))
frequency <- as.data.frame(c(10,40,40,160,18,132,18,132,8,192,2,48))
data <- cbind(social_economics,Boy_Scout,deliquency,frequency)
#input the data
social_economics <- c(rep('low',4),rep('Median',4),rep('high',4))
Boy_Scout <- c(rep(c(rep('Yes',2),rep('No',2)),3))
deliquency <- c(rep(c('Yes','No'),6))
frequency <- c(10,40,40,160,18,132,18,132,8,192,2,48)
data <- as.data.frame(cbind(social_economics,Boy_Scout,deliquency,frequency))
View(data)
social_economics <- c(rep('low',2),rep('Median',2),rep('high',2))
Boy_Scout <- c(rep(c(rep('Yes',1),rep('No',1)),3))
deliquency_yes <- c(10,40,18,18,8,2)
deliquency_no <- c(40,160,132,132,192,48)
data_1 <- as.data.frame(cbind(social_economics,Boy_Scout,deliquency,frequency))
data_1
social_economics <- c(rep('low',2),rep('Median',2),rep('high',2))
Boy_Scout <- c(rep(c(rep('Yes',1),rep('No',1)),3))
deliquency_yes <- c(10,40,18,18,8,2)
deliquency_no <- c(40,160,132,132,192,48)
data_1 <- as.data.frame(cbind(social_economics,Boy_Scout,deliquency,frequency))
data_1
social_economics <- c(rep('low',2),rep('Median',2),rep('high',2))
Boy_Scout <- c(rep(c(rep('Yes',1),rep('No',1)),3))
deliquency_yes <- c(10,40,18,18,8,2)
deliquency_no <- c(40,160,132,132,192,48)
data_1 <- as.data.frame(cbind(social_economics,Boy_Scout,deliquency_yes,deliquency_no))
data_1
logit1 <- glm(formula = cbind(deliquency_yes,deliquency_no)~factor(social_economics)+factor(Boy_Scout),family = binomial)
logit1
logit1 <- glm(formula = cbind(deliquency_yes,deliquency_no)~factor(social_economics)+factor(Boy_Scout),family = binomial)
logit1
summary(logit1)
logit1 <- glm(formula = cbind(deliquency_yes,deliquency_no)~factor(social_economics)+factor(Boy_Scout),family = binomial)
summary(logit1)
confint(logit1)
logit1 <- glm(formula = cbind(deliquency_yes,deliquency_no)~factor(social_economics)+factor(Boy_Scout),family = binomial)
summary(logit1)
confint(logit1)
social_economics <- factor(c(rep('low',2),rep('Median',2),rep('high',2)))
Boy_Scout <- factor(c(rep(c(rep('Yes',1),rep('No',1)),3)))
deliquency_yes <- c(10,40,18,18,8,2)
deliquency_no <- c(40,160,132,132,192,48)
data_1 <- as.data.frame(cbind(social_economics,Boy_Scout,deliquency_yes,deliquency_no))
data_1
logit1 <- glm(formula = cbind(deliquency_yes,deliquency_no)~factor(social_economics)+factor(Boy_Scout),family = binomial)
summary(logit1)
confint(logit1)
exp(-3.178)
exp(1.792)
exp(1.186)
exp(-3.956e-16)
confint(logit1)
exp(-3.9776309)
exp(-2.4766143)
exp(1.0633292)
exp(2.6037768)
exp(1.9723094)
exp(0.4832948)
exp(0.4897023)
exp(-0.4971858)
social_economics <- factor(c(rep('low',2),rep('Median',2),rep('high',2)))
deliquency <- factor(c(rep(c(rep('Yes',1),rep('No',1)),3)))
boy_scout_yes <- c(10,40,18,18,8,2)
boy_scout_no <- c(40,160,132,132,192,48)
data_2 <- as.data.frame(cbind(social_economics,Boy_Scout,deliquency_yes,deliquency_no))
data_2
social_economics <- factor(c(rep('low',2),rep('Median',2),rep('high',2)))
deliquency <- factor(c(rep(c(rep('Yes',1),rep('No',1)),3)))
boy_scout_yes <- c(10,40,18,18,8,2)
boy_scout_no <- c(40,160,132,132,192,48)
data_2 <- as.data.frame(cbind(social_economics,deliquency,boy_scout_yes,boy_scout_no))
data_2
social_economics <- factor(c(rep('low',2),rep('Median',2),rep('high',2)))
deliquency <- factor(c(rep(c(rep('Yes',1),rep('No',1)),3)))
boy_scout_yes <- c(10,40,18,132,8,192)
boy_scout_no <- c(40,160,18,132,2,48)
data_2 <- as.data.frame(cbind(social_economics,deliquency,boy_scout_yes,boy_scout_no))
data_2
logit2 <- glm(formula = cbind(boy_scout_yes,boy_scout_no)~factor(social_economics)+factor(deliquency),family = binomial)
summary(logit2)
exp(1.386)
exp(-2.773)
exp(-1.386)
exp(-9.279e-16)
confint(logit1)
confint(logit2)
exp(1.7071188)
exp(.0846875)
exp(-3.2281262)
exp(-2.3365690)
exp(-1.7788290)
exp(-1.0062695\)
exp(-1.0062695)
knitr::opts_chunk$set(echo = TRUE)
library(mvtnorm)
FT = 0.0346
C = pmvnorm(lower=c(-Inf,-Inf),upper=c(qnorm(FT),qnorm(FT)),sigma=matrix(c(1,0.2,0.2,1),2,2))
C
1000000*exp(-0.04*1)*(FT+FT-C)
library(mvtnorm)
FT = 0.0346
C = pmvnorm(lower=c(-Inf,-Inf),upper=c(qnorm(FT),qnorm(FT)),sigma=matrix(c(1,0.2,0.2,1),2,2))
C
1000000*exp(-0.04*1)*(FT+FT-C)
1000000*exp(-0.04*5)*(FT+FT-C)
library(mvtnorm)
FT = 0.047
C = pmvnorm(lower=c(-Inf,-Inf),upper=c(qnorm(FT),qnorm(FT)),sigma=matrix(c(1,0.2,0.2,1),2,2))
C
1000000*exp(-0.04*5)*(FT+FT-C)
library(mvtnorm)
FT = 0.047
C = pmvnorm(lower=c(-Inf,-Inf),upper=c(qnorm(FT),qnorm(FT)),sigma=matrix(c(1,0.2,0.2,1),2,2))
1000000*exp(-0.04*5)*(FT+FT-C)
C = pmvnorm(lower=c(-Inf,-Inf),upper=c(qnorm(FT),qnorm(FT)),sigma=matrix(c(1,0.2,0.2,1),2,2))
C
1000000*exp(-0.04*5)*(FT+FT-C)
1000000*exp(-0.04*1)*(FT+FT-C)
1000000*exp(-0.04*1)*(C)
1-exp(-0.01 * 1)
1-exp(-0.02 * 1)
copula=archmCopula(family="gumbel", 2))
??gumbel
exp(-((-log(1-exp(-0.01 * 1)))^2 + (-log(1-exp(-0.02 * 1)))^2 )^(1/2) )
1-exp(-0.01 * 1) + 1-exp(-0.02 * 1) -  exp(-((-log(1-exp(-0.01 * 1)))^2 + (-log(1-exp(-0.02 * 1)))^2 )^(1/2) )
exp(-((-log(1-exp(-0.01 * 1)))^2 + (-log(1-exp(-0.02 * 1)))^2 )^(1/2) ) * 1000000 * exp(-1 * 0 *1)
(1-exp(-0.01 * 1)) * (1-exp(-0.02 * 1)) * 1000000 * exp(-1 * 0 *1)
-0.019+0.025*70*0.544+0.5*0.22^2*70*0.052
0.025*3.26
-0.019+0.025*70*0.544+0.5*0.22^2*70^2*0.052
pnorm(1,mean = 0,sd=1,lower.tail = FALSE)
qnorm(0.1,mean = 0,sd=1,lower.tail = FALSE)
qnorm(0.1,mean = 0,sd=1,lower.tail = FALSE) * 0.016 -0.002
qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE)
qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * 0.2 - 0.1
1000000*(qnorm(0.1,mean = 0,sd=1,lower.tail = FALSE) * 0.016 -0.002 )
1000*(qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * 0.2 - 0.1)
qt(0.1,df = 2,lower.tail = FALSE)
1000 * (qt(0.1,df = 2,lower.tail = FALSE)*0.016 - 0.002)
qt(0.1,df = 5,lower.tail = FALSE)
1000 * (qt(0.1,df = 5,lower.tail = FALSE)*0.016 - 0.002)
500000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * 0.15 - 0.05)*@
500000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * 0.15 - 0.05)*2
500000 * (qnorm(0.025,mean = 0,sd=1,lower.tail = FALSE) * 0.15 - 0.05)*2
500000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * 0.15 - 0.05)*2
500000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * 0.15 - 0.05)*2 + (500000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * 0.15 - 0.05))^2 * 0.5
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * (0.5 * 0.15) - 0.05)
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * (0.5 * 0.15 + 2 * 0.5 * 0.5 * 0.5) - 0.05)
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * sqrt(0.25*0.15*2) - 0.05)
0.15^2*0.25*2
-sqrt(0.01125)*(qnorm(0.05))-0.05
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * sqrt(0.15^2*0.25*2) - 0.05)
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * sqrt(0.15^2*0.25*2 + 0.25*0.5^2*2) - 0.05)
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * sqrt(0.15^2*0.25*2 - 0.25*0.5^2*2) - 0.05)
0.5 * sqrt(0.15^2*2)
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * sqrt(0.15^2*0.25*2 - 0.25*0.106066^2*2) - 0.05)
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * sqrt(0.15^2*0.25*2 + 0.25*0.106066^2*2) - 0.05)
(1/3)*0.01 + (2/3)*0.005
(1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2
sqrt((1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2)
qnorm(0.05,lower.tail = FALSE)
1500 * (sqrt((1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2)*qnorm(0.05,lower.tail = FALSE) - (1/3)*0.01 + (2/3)*0.005)
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * sqrt(0.15^2*0.25*2 - 0.5 * sqrt(0.15^4)) - 0.05)
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * sqrt(0.15^2*0.25*2 - 0.5 * sqrt(0.15^2*2)) - 0.05)
1000000 * (qnorm(0.05,mean = 0,sd=1,lower.tail = FALSE) * sqrt(0.15^2*0.25*2 - 0.25*(0.5 * sqrt(0.15^2*4))^2*2) - 0.05)
0.3 * 0.05 * 0.01
sqrt((1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2 + 2*(1/3)*(2/3)*0.3 * 0.05 * 0.01)
(1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2 + 2*(1/3)*(2/3)*0.3 * 0.05 * 0.01
sqrt((1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2 + 2*(1/3)*(2/3)*0.3 * 0.05 * 0.01)
1500 * (sqrt((1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2)*qnorm(0.05,lower.tail = FALSE) - ((1/3)*0.01 + (2/3)*0.005))
1500 * (sqrt((1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2 + 2*(1/3)*(2/3)*0.3 * 0.05 * 0.01)*qnorm(0.05,lower.tail = FALSE) - ((1/3)*0.01 + (2/3)*0.005))
sqrt((1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2 - 2*(1/3)*(2/3)*0.3 * 0.05 * 0.01)
(1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2 - 2*(1/3)*(2/3)*0.3 * 0.05 * 0.01
1500 * (sqrt((1/3)^2 *(0.05)^2 + (2/3)^2 * 0.01 ^2 - 2*(1/3)*(2/3)*0.3 * 0.05 * 0.01)*qnorm(0.05,lower.tail = FALSE) - ((1/3)*0.01 + (2/3)*0.005))
pi
cos(-pi/2)
log(cos(-pi/2))
log(cos(-pi/4))
37.33189-0.3465736
log(cos(pi/2)) - log(cos(-pi/4))
pi/4 + 1
4/ (pi + 4)
uniroot?
?uniroot
uniroot((4/(pi+4) + (pi/4 + cos(2*x)/4 + x/2 + sin(2*x)/4) - 0.05),c(0,10))
f <- function(x){(4/(pi+4) + (pi/4 + cos(2*x)/4 + x/2 + sin(2*x)/4) - 0.05)}
uniroot(f,c(0,10))
uniroot(f,c(-10,10))
f <- function(x){(4/(pi+4))*(pi/4 + cos(2*x)/4 + x/2 + sin(2*x)/4) - 0.05)}
f <- function(x){(4/(pi+4))*(pi/4 + cos(2*x)/4 + x/2 + sin(2*x)/4) - 0.05}
uniroot(f,c(-10,10))
f <- function(x){(4/(pi+4))*(pi/4 + cos(2*x)/4 - x/2 - sin(2*x)/4) - 0.05}
uniroot(f,c(-10,10))
integrate(f,lower = - Inf,upper = tan(0.840397))
integrate(f,lower = - Inf,upper = tan(0.840397))
pi/4 = 0.05
pi/4 - 0.05
f <- function(x){(4/(pi+4))*(pi/4 + cos(x)^2/2 - x/2 - sin(2*x)/4) - 0.05}
uniroot(f,c(-10,10))
?rnorm
100000 * ((0.18* dnorm(qnorm(0.05,lower.tail = ))))
100000 * ((0.18* dnorm(qnorm(0.05,lower.tail = FALSE))/0.05 + 0.04  ))
0.2 * 0.18 * 0.18
(0.5^2 * 0.18 ^ 2)*2 + (0.5^2 * 0.00648) *2
sqrt(0.01944)
100000 * ((0.1394274* dnorm(qnorm(0.05,lower.tail = FALSE))/0.05 + 0.04  ))
0.11 - 0.03
0.08 * (0.12/(0.14-0.03))
24/275
0.023 + (0.1 - 0.023)*0.05/0.12
661/12000
install.packages("e1071")
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
2*(1-pbinom(10,12,0.5))
2*(1-pbinom(11,12,0.5))
income <- c(7，1110，7.1,5.2,8,12,0,5,2.1,2,46,7.5)
library(bsda)
library(BSDA)
library('BSDA')
install.packages('BSDA')
SIGN.test(income,md=1,alternative = 'two.sided')
pbinom(25,0.691,lower.tail = FALSE)
pbinom(q = 16,25,0.691,lower.tail = FALSE)
pbinom(q = 16, 25,0.5)
pbinom(q = 16, 25,0.5,lower.tail = FALSE)
pbinom(q = 16, 25,0.5,lower.tail = FALSE)*2
pbinom(q = 16, size = 25,prob = 0.5,lower.tail = FALSE)*2
pbinom(q = 16, size = 25,prob = 0.5,lower.tail = FALSE)
pbinom(q = 16, size = 25,prob = 0.691,lower.tail = FALSE)
setwd("C:/Users/cheny/Desktop/study/second term/staistical machine learning/homework/homework four")
knitr::opts_chunk$set(echo = TRUE)
train_3 <- read.table("train_3.txt", header=FALSE, sep=",")
train_3$number <- 3
train_5 <- read.table("train_5.txt", header=FALSE, sep=",")
train_5$number <- 5
train_8 <- read.table("train_8.txt", header=FALSE, sep=",")
train_8$number <- 8
train_data <- rbind(train_3,train_5,train_8)
train_3 <- read.table("train_3.txt", header=FALSE, sep=",")
train_3$number <- 3
train_5 <- read.table("train_5.txt", header=FALSE, sep=",")
train_5$number <- 5
train_8 <- read.table("train_8.txt", header=FALSE, sep=",")
train_8$number <- 8
train_data <- rbind(train_3,train_5,train_8)
test <- read.table("zip_test.txt",header = FALSE,sep = " ")
number <- test[,1]
test_data <- test[,-1]
test_data$number <- number
colnames(test_data) <- c("V1",colnames(test_data)[-256])
# we only need the testing data which is represent the number 3,5,8
test_data <- test_data[(test_data$number==3 | test_data$number==5|test_data$number==8),
test <- read.table("zip_test.txt",header = FALSE,sep = " ")
number <- test[,1]
test_data <- test[,-1]
test_data$number <- number
colnames(test_data) <- c("V1",colnames(test_data)[-256])
# we only need the testing data which is represent the number 3,5,8
test_data <- test_data[(test_data$number==3 | test_data$number==5|test_data$number==8),]
sample.int(10)
knitr::opts_chunk$set(echo = TRUE)
n<-nrow(X)
knitr::opts_chunk$set(echo = TRUE)
# 1.Decision stumps
train<-function(x,w,y){
n<-nrow(x)
p<-ncol(x)
met<-matrix(nrow=p)
theta<-matrix(nrow=p)
loss<-matrix(nrow=p)
for (j in 1:p){
index<-order(x[,j])
x_j<-x[index,j]
w_cum<-cumsum(w[index]*y[index]) # compute cumulative sum
w_cum[duplicated(x_j)==1]<-NA # multiple occurrences of same x_j value
# optimal threshold
m<-max(abs(w_cum), na.rm=TRUE)
maxIndex<-min(which(abs(w_cum)==m))
met[j]<-(w_cum[maxIndex]<0)*2 - 1
theta[j] <- x_j[maxIndex]
c <- ((x_j > theta[j])*2 - 1) * met[j]
loss[j]<-w %*% (c!=y)
}
m<-min(loss)
j_opt<-min(which(loss==m))
pars<-list(j=j_opt,theta=theta[j_opt],mode=met[j_opt])
return(pars)
}
classify<-function(x,pars){
j <- pars$j
t <- pars$theta
m <- pars$mode
l <- x[,j]
pred <- m * (l-t)
pred[pred < 0] <- -1
pred[pred >= 0] <- 1
return(pred)
}
# 1.AdaBoost algorithm
adaboost<-function(x,y,B){
alpha<-rep(0,B)
allPars<-rep(list(list()),B)
n<-nrow(x)
w<-rep(1/n, times=n) # for the first round we that all the weight as 1/w
for (b in 1:B){
allPars[[b]]<-train(x,w,y) # train base classifier
missclass<-as.numeric(y!=classify(x,allPars[[b]])) # error
e<-(w%*%missclass/sum(w))[1]
alpha[b]<-log((1-e)/e) # voting weight
w<-w*exp(alpha[b]*missclass) # recompute weight
}
return(list(allPars=allPars,alpha=alpha))
}
# evaluate aggregated classifier on x
agg_class<-function(x,alpha,allPars){
n<-nrow(x)
B<-length(alpha)
labels<-matrix(0,nrow=n,ncol=B)
for(b in 1:B){
labels[,b]<-classify(x,allPars[[b]])
}
labels<-labels %*% alpha
c_hat<-sign(labels)
return(c_hat)
}
# 3.Run algorithm on USPS data, evaluate results using cross validation
train.3<-read.table("train_3.txt",header = FALSE, sep=",")
train.8<-read.table("train_8.txt",header = FALSE, sep=",")
xtrain<-rbind(as.matrix(train.3),as.matrix(train.8))
ytrain<-as.matrix(rep(c(-1,1),c(nrow(train.3),nrow(train.8))))
test<-as.matrix(read.table("zip_test.txt"))
ytest<-test[,1]
xtest<-test[ytest==3|ytest==8,-1]
ytest<-as.matrix(ytest[ytest==3|ytest==8])
ytest[ytest==3]<--1
ytest[ytest==8]<-1
# combine train and test for future cv
X<-rbind(xtrain,xtest)
Y<-rbind(ytrain,ytest)
n<-nrow(X)
B_max<-100
nCV<-5
set.seed(1)
testErrorRate<-matrix(0,nrow=B_max,ncol=nCV)
trainErrorRate<-matrix(0,nrow=B_max,ncol=nCV)
p <- sample.int(n)
for (i in 1:nCV){
trainIndex<-p[(i-1)*round(n/5)+1:i*round(n/5)]
testIndex<-p[-(i:round(n/5))]
ada<-adaboost(X[trainIndex,],Y[trainIndex],B_max)
allPars<-ada$allPars
alpha<-ada$alpha
# error rate
for(B in 1:B_max){
c_hat_test<-agg_class(X[testIndex,],alpha[1:B],allPars[1:B])
testErrorRate[B,i]<-mean(Y[testIndex] != c_hat_test)
c_hat_train<-agg_class(X[trainIndex,],alpha[1:B],allPars[1:B])
trainErrorRate[B,i]<-mean(Y[trainIndex] != c_hat_train)
}
}
n<-nrow(X)
B_max<-100
nCV<-5
set.seed(1)
testErrorRate<-matrix(0,nrow=B_max,ncol=nCV)
trainErrorRate<-matrix(0,nrow=B_max,ncol=nCV)
p <- sample.int(n)
for (i in 1:nCV){
trainIndex<-p[(i-1)*round(n/5)+1:i*round(n/5)]
testIndex<-p[-(i:round(n/5))]
ada<-adaboost(X[trainIndex,],Y[trainIndex],B_max)
allPars<-ada$allPars
alpha<-ada$alpha
# error rate
for(B in 1:B_max){
c_hat_test<-agg_class(X[testIndex,],alpha[1:B],allPars[1:B])
testErrorRate[B,i]<-mean(Y[testIndex] != c_hat_test)
c_hat_train<-agg_class(X[trainIndex,],alpha[1:B],allPars[1:B])
trainErrorRate[B,i]<-mean(Y[trainIndex] != c_hat_train)
}
}
n<-nrow(X)
B_max<-100
nCV<-5
set.seed(1)
testErrorRate<-matrix(0,nrow=B_max,ncol=nCV)
trainErrorRate<-matrix(0,nrow=B_max,ncol=nCV)
p <- sample.int(n)
for (i in 1:nCV){
trainIndex<-p[((i-1)*round(n/5)+1:i*round(n/5))]
testIndex<-p[-(i:round(n/5))]
ada<-adaboost(X[trainIndex,],Y[trainIndex],B_max)
allPars<-ada$allPars
alpha<-ada$alpha
# error rate
for(B in 1:B_max){
c_hat_test<-agg_class(X[testIndex,],alpha[1:B],allPars[1:B])
testErrorRate[B,i]<-mean(Y[testIndex] != c_hat_test)
c_hat_train<-agg_class(X[trainIndex,],alpha[1:B],allPars[1:B])
trainErrorRate[B,i]<-mean(Y[trainIndex] != c_hat_train)
}
}
p <- sample.int(n)
p
p[((i-1)*round(n/5)+1:i*round(n/5))]
p[(i-1)*round(n/5)+1:i*round(n/5)]
(i-1)*round(n/5)+1:i*round(n/5)
(i-1)*round(n/5)+1
i*round(n/5)
p[1:306]
trainIndex<-p[((i-1)*round(n/5)+1):(i*round(n/5))]
trainIndex
n<-nrow(X)
B_max<-100
nCV<-5
set.seed(1)
testErrorRate<-matrix(0,nrow=B_max,ncol=nCV)
trainErrorRate<-matrix(0,nrow=B_max,ncol=nCV)
p <- sample.int(n)
for (i in 1:nCV){
trainIndex<-p[((i-1)*round(n/5)+1):(i*round(n/5))]
testIndex<-p[-(((i-1)*round(n/5)+1):(i*round(n/5)))]
ada<-adaboost(X[trainIndex,],Y[trainIndex],B_max)
allPars<-ada$allPars
alpha<-ada$alpha
# error rate
for(B in 1:B_max){
c_hat_test<-agg_class(X[testIndex,],alpha[1:B],allPars[1:B])
testErrorRate[B,i]<-mean(Y[testIndex] != c_hat_test)
c_hat_train<-agg_class(X[trainIndex,],alpha[1:B],allPars[1:B])
trainErrorRate[B,i]<-mean(Y[trainIndex] != c_hat_train)
}
}
# 4. Plot train error and test error
matplot(trainErrorRate,type="l",lty=1:nCV,main="Cross Validation Training Error",xlab="Number of base classifiers",ylab="error rate")
matplot(testErrorRate,type="l",lty=1:nCV,main="Cross Validation Test Error",xlab="Number of base classifiers",ylab="error rate")
# sum up the validation error rate for different B
Average_testErrorRate <- apply(testErrorRate,1,FUN = mean)
plot(Average_testErrorRate,type="l",lty=1:nCV,main="Average Validation Error",xlab="Number of base classifiers",ylab="error rate")
library(ada)
adaboost(X[trainIndex,],Y[trainIndex],B = 1)
library(freestats)
adaBoost(X[trainIndex,],Y[trainIndex])
adaBoost(dat.train = X[trainIndex,],y.train = Y[trainIndex])
adaboost(X,Y,B = 1)
testErrorRate
trainErrorRate
