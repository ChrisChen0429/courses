{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Machine Learning\n",
    "## Homework 1\n",
    "- Name: **YI CHEN**\n",
    "- UNI: **yc3356**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem One(Maximum Likelihood Estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. general analytic procedure to obtain the maximum likelihood estimator\n",
    " #### example distribution: gamma distribution\n",
    " - first: \n",
    " \n",
    " find the conditional distribution of every single data $x_i$ given the parameter $\\theta$ : \n",
    "     \n",
    "     $p(x_i | \\theta)= (\\frac{v}{\\mu})^{v} \n",
    "     \\frac{{x_i}^{v-1}}{\\Gamma(v)} e^{- \\frac{v x_i}{\\mu}}$ \n",
    " - second:\n",
    " suppose that the data is independent and identically distributed (i.i.d). We can find the condictional distritbuion of all the data given the parameter $\\theta$: \n",
    "     \n",
    "     $p(x_1,...,x_n | \\theta) = \\prod_{i=1}^{n}p(x_i | \\theta)= (\\frac{v}{\\mu})^{nv} \\frac{\\prod_{i=1}^{n}{x_i}^{v-1}}{\\Gamma(v)^{n}} e^{- \\frac{v}{\\mu} \\sum_{i=1}^{n}}$\n",
    " - thrid:\n",
    " we take the condictional distritbuion of all the data given the parameter $\\theta$ as a function of parameter $\\theta$, since we have all the data. Here this function is called likelihood function. And our goal is : ** find the paramter $\\theta$ that maximize the likelihood function**. Here we use **logarithm trick** to avoid product rule computation:\n",
    " \n",
    " $L(\\theta) = (\\frac{v}{\\mu})^{nv} \\frac{\\prod_{i=1}^{n}{x_i}^{v-1}}{\\Gamma(v)^{n}} e^{- \\frac{v}{\\mu} \\sum_{i=1}^{n}}$\n",
    " \n",
    " $\\Rightarrow$ $l(\\theta) = nv log(v) - nv log(\\mu) + (v-1) \\sum_{i=1}^{n} log(x_i) - nlog(\\Gamma(v)) - (\\frac{v}{u}) \\sum_{i=1}^{n}x_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2. derive the ML estimator for the location parameter $\\mu$\n",
    " \n",
    " - As we have already know, the log likelihood function is look like this:\n",
    " \n",
    " $l(\\theta) = nv log(v) - nv log(\\mu) + (v-1) \\sum_{i=1}^{n} log(x_i) - (\\frac{v}{u}) \\sum_{i=1}^{n}x_i $ \n",
    " \n",
    " Here the only paramter we need to estimate is $\\mu$, thus, for the log likelihood function, the real parameter is $\\mu$. And for the terms that do not related with $\\mu$, we can just take them as a constant.\n",
    " \n",
    " - Calculate the ML estimator of $\\mu$:\n",
    " \n",
    " $\\hat{\\mu}_{ML} = argmax_{\\mu} l(\\theta)$\n",
    " \n",
    " To solve this:\n",
    " \n",
    " $\\bigtriangledown_{\\mu}l(\\mu) = 0 \\Rightarrow \\frac{-nv}{\\mu} + \\frac{v}{u^2 \\sum_{i=1}^{n}x_i} = 0$\n",
    " \n",
    " Now we get: $\\mu = \\frac{1}{\\bar{x_n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. derive the ML estimator for the location parameter $v$ \n",
    "\n",
    "- First we already know:\n",
    "\n",
    "$l(\\theta) = nv log(v) - nv log(\\mu) + (v-1) \\sum_{i=1}^{n} log(x_i) - nlog(\\Gamma(v)) - (\\frac{v}{u}) \\sum_{i=1}^{n}x_i $\n",
    "\n",
    "    Here we take $v$ as the unknow parameter\n",
    "- Second we find the solution through derivation:\n",
    "\n",
    "$\\bigtriangledown_{v}l(v) = 0$  $\\Rightarrow$ $nlog(v) + n - nlog(\\mu) + \\sum_{i=1}^{n}log(x_i) - n \\frac{\\frac{\\partial \\Gamma(v) }{\\partial v }}{\\Gamma(v)} - \\frac{\\sum_{i=1}^{n}x_i}{\\mu}=0$ \n",
    "\n",
    "$\\Rightarrow$  $\\sum_{i=1}^{n}(ln(\\frac{x_i}{\\hat{v}}) - (\\frac{x_i}{\\mu}-1) - \\phi(\\hat{v})) = 0$, where $\\phi(v) := \\frac{\\frac{\\partial \\Gamma(v) }{\\partial v }}{\\Gamma(v)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 (Bayes-Optimal Classier)\n",
    " To show taht the Bayes-optimal classifier is the classifier which minimizes the probability of error, under all classifier in the hypothesis class\n",
    " \n",
    " - The probability of error is the risk under zero-one loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
