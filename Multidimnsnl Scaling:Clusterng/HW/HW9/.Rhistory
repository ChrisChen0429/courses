gb = apply((1-etaim)*(1-Yt),1,sum)
sa = apply(etaim*(1-Yt),1,sum)
sb = apply(etaim*Yt,1,sum)
g = qbeta(runif(J, 0,pbeta(1-s,1+ga,1+gb)),1+ga,1+gb)
s = qbeta(runif(J,0,pbeta(1-g,1+sa,1+sb)),1+sa,1+sb)
g.out = rbind(g.out,g)
s.out = rbind(s.out,s)
Q = t(sapply(1:J,function(j){sample.Q(all.a[-1,],g[j],s[j],Y[,j],alpha,p[j,])}))
Q.out = rbind(Q.out,Q)
pa = a + Q; pb = a +1-Q
ppp = rbeta(J*K, pa, pb)
p = matrix(ppp, J, K)
p.out = cbind(p.out, p)
}
p.out = array(p.out, c(J,K,niter))
Q.out = array(Q.out, c(J,niter,K))
out = list(pi=pi.out, Q=Q.out, g=g.out, s=s.out, p=p.out)
class(out) = 'cdmcmc'
out
}
sample.Q = function(all.a, g, s, Y, alpha,pp){
natt = apply(all.a,1,sum)
cc = tcrossprod(all.a,alpha)
etaim = (cc==natt) # all the attribute patterns smaller than alpha will be true other wise is false
pp[pp<1e-8] = 1e-8
pp[pp>1-1e-8] = 1-1e-8
pp = all.a%*%log(pp) + (1-all.a)%*%log(1-pp)
ga = (1-etaim)%*%Y
gb = (1-etaim)%*%(1-Y)
sa = etaim%*%(1-Y)
sb = etaim%*%Y
pm = ga*log(g) + gb*log(1-g) + sa*log(s) + sb*log(1-s)
pm = pm + pp
pm = pm - max(pm)
pm = as.vector(exp(pm))
pm = pm/sum(pm)
kk = nrow(all.a)
q = sample(1:kk,size=1,prob=pm)
q = (as.binary(c(q,kk))[1,])
q
}
system.time(out <- EstQMCMC(yy, K=4, niter=100000))
system.time(out <- EstQMCMC(yy, K=4, niter=100))
yy
K=4
niter=100
N = nrow(Y)
J = ncol(Y)
Y = as.matrix(Y)
if(is.null(K) & is.null(q.start))
stop('User must supply either the number of attributes or a starting Q matrix!\n\n')
Q = matrix(rbinom(J*K, 1, 0.5), J, K)
Q[which(apply(Q, 1, sum)==0), 1] = 1
g = runif(J, 0.1, 0.3)
s = runif(J, 0.1, 0.3)
pi = exp(rnorm(2^K))
pi = pi/sum(pi)
pi = pi.start
all.a = as.binary(0:(2^K-1))
natt = apply(Q,1,sum)
Yt = t(Y)
a <- matrix(NA,nrow = N,ncol = K)
for (i in 1:N){
C <- sample(x = 1:2^K,size = 1,prob = pi)
a[i,] = all.a[C,]
}
p = ifelse(Q==1, 0.6, 0.4)
pi.out = pi; Q.out = Q; p.out = p; g.out = g; s.out = s
for(ii in 1:niter){
etajm = tcrossprod(Q,all.a) ## number of required attribute mastered for each attribute pattern
natt = apply(Q,1,sum)  ## number of attribute required by each item
etajm = (etajm == natt)    ## whether or not the required pattern fully master the attributes required
pp = g*(1-etajm) + (1-s)*etajm ## probability of each attribute pattern will make the correct response
ll = Y %*% log(pp) + (1-Y)%*%log(1-pp) ## log likelihood of the respone
ll = sweep(ll,2,log(pi),'+') ## add the prior of attribute pattern into the ll
pp = exp(ll) # make it back to the original scale (N * 2^K)
pp = apply(pp,1,cumsum) ## cunsum the proability for each subject over the attribute pattern (2^K * N)
pp = sweep(pp,2,pp[2^K,],'/') ## put the probabilty into the probability scale and ensure the monotocity
## however, in this way the last attribute pattern always has the probability as 1
u = runif(N)
alpha = apply(sweep(pp,2,u,'<'),2,sum)
alpha = as.binary(c(2^K-1,alpha))[-1,]  # generate the attribute profile
cc = as.vector(alpha%*%(2^((K-1):0)))
cc = apply(outer(cc,0:(2^K-1),'=='),2,sum) ## observed proportation of attribute patterns
pi = rgamma(2^K, 1+cc)
pi = pi/sum(pi)
pi.out = rbind(pi.out,pi) # p vector updated!
####### update the slipping and guessing ###########
etaim = tcrossprod(Q,alpha)
etaim = (etaim == natt)
ga = apply((1-etaim)*Yt,1,sum)
gb = apply((1-etaim)*(1-Yt),1,sum)
sa = apply(etaim*(1-Yt),1,sum)
sb = apply(etaim*Yt,1,sum)
g = qbeta(runif(J, 0,pbeta(1-s,1+ga,1+gb)),1+ga,1+gb)
s = qbeta(runif(J,0,pbeta(1-g,1+sa,1+sb)),1+sa,1+sb)
g.out = rbind(g.out,g)
s.out = rbind(s.out,s)
Q = t(sapply(1:J,function(j){sample.Q(all.a[-1,],g[j],s[j],Y[,j],alpha,p[j,])}))
Q.out = rbind(Q.out,Q)
pa = a + Q; pb = a +1-Q
ppp = rbeta(J*K, pa, pb)
p = matrix(ppp, J, K)
p.out = cbind(p.out, p)
}
ii =1
etajm = tcrossprod(Q,all.a) ## number of required attribute mastered for each attribute pattern
etajm
natt = apply(Q,1,sum)  ## number of attribute required by each item
natt
etajm = (etajm == natt)    ## whether or not the required pattern fully master the attributes required
pp = g*(1-etajm) + (1-s)*etajm ## probability of each attribute pattern will make the correct response
ll = Y %*% log(pp) + (1-Y)%*%log(1-pp) ## log likelihood of the respone
ll = sweep(ll,2,log(pi),'+') ## add the prior of attribute pattern into the ll
pp = exp(ll) # make it back to the original scale (N * 2^K)
pp = apply(pp,1,cumsum) ## cunsum the proability for each subject over the attribute pattern (2^K * N)
pp = sweep(pp,2,pp[2^K,],'/') ## put the probabilty into the probability scale and ensure the monotocity
## however, in this way the last attribute pattern always has the probability as 1
u = runif(N)
alpha = apply(sweep(pp,2,u,'<'),2,sum)
alpha = as.binary(c(2^K-1,alpha))[-1,]  # generate the attribute profile
cc = as.vector(alpha%*%(2^((K-1):0)))
cc = apply(outer(cc,0:(2^K-1),'=='),2,sum) ## observed proportation of attribute patterns
pi = rgamma(2^K, 1+cc)
pi = pi/sum(pi)
pi.out = rbind(pi.out,pi) # p vector updated!
####### update the slipping and guessing ###########
etaim = tcrossprod(Q,alpha)
etaim = (etaim == natt)
ga = apply((1-etaim)*Yt,1,sum)
gb = apply((1-etaim)*(1-Yt),1,sum)
sa = apply(etaim*(1-Yt),1,sum)
sb = apply(etaim*Yt,1,sum)
g = qbeta(runif(J, 0,pbeta(1-s,1+ga,1+gb)),1+ga,1+gb)
s = qbeta(runif(J,0,pbeta(1-g,1+sa,1+sb)),1+sa,1+sb)
g.out = rbind(g.out,g)
s.out = rbind(s.out,s)
Q = t(sapply(1:J,function(j){sample.Q(all.a[-1,],g[j],s[j],Y[,j],alpha,p[j,])}))
Q
Q.out = rbind(Q.out,Q)
pa = a + Q; pb = a +1-Q
a
Q
dim(a)
dim(Q)
pa = a + Q
pb = a +1-Q
1-Q
Q
a.start =1
#a <- matrix(NA,nrow = N,ncol = K)
#for (i in 1:N){
#  C <- sample(x = 1:2^K,size = 1,prob = pi)
#  a[i,] = all.a[C,]
#}
a <- a.start
pa = a + Q
pb = a +1-Q
ppp = rbeta(J*K, pa, pb)
p = matrix(ppp, J, K)
p.out = cbind(p.out, p)
for(ii in 1:niter){
etajm = tcrossprod(Q,all.a) ## number of required attribute mastered for each attribute pattern
natt = apply(Q,1,sum)  ## number of attribute required by each item
etajm = (etajm == natt)    ## whether or not the required pattern fully master the attributes required
pp = g*(1-etajm) + (1-s)*etajm ## probability of each attribute pattern will make the correct response
ll = Y %*% log(pp) + (1-Y)%*%log(1-pp) ## log likelihood of the respone
ll = sweep(ll,2,log(pi),'+') ## add the prior of attribute pattern into the ll
pp = exp(ll) # make it back to the original scale (N * 2^K)
pp = apply(pp,1,cumsum) ## cunsum the proability for each subject over the attribute pattern (2^K * N)
pp = sweep(pp,2,pp[2^K,],'/') ## put the probabilty into the probability scale and ensure the monotocity
## however, in this way the last attribute pattern always has the probability as 1
u = runif(N)
alpha = apply(sweep(pp,2,u,'<'),2,sum)
alpha = as.binary(c(2^K-1,alpha))[-1,]  # generate the attribute profile
cc = as.vector(alpha%*%(2^((K-1):0)))
cc = apply(outer(cc,0:(2^K-1),'=='),2,sum) ## observed proportation of attribute patterns
pi = rgamma(2^K, 1+cc)
pi = pi/sum(pi)
pi.out = rbind(pi.out,pi) # p vector updated!
####### update the slipping and guessing ###########
etaim = tcrossprod(Q,alpha)
etaim = (etaim == natt)
ga = apply((1-etaim)*Yt,1,sum)
gb = apply((1-etaim)*(1-Yt),1,sum)
sa = apply(etaim*(1-Yt),1,sum)
sb = apply(etaim*Yt,1,sum)
g = qbeta(runif(J, 0,pbeta(1-s,1+ga,1+gb)),1+ga,1+gb)
s = qbeta(runif(J,0,pbeta(1-g,1+sa,1+sb)),1+sa,1+sb)
g.out = rbind(g.out,g)
s.out = rbind(s.out,s)
Q = t(sapply(1:J,function(j){sample.Q(all.a[-1,],g[j],s[j],Y[,j],alpha,p[j,])}))
Q.out = rbind(Q.out,Q)
pa = a + Q
pb = a +1-Q
ppp = rbeta(J*K, pa, pb)
p = matrix(ppp, J, K)
p.out = cbind(p.out, p)
}
p.out = array(p.out, c(J,K,niter))
Q.out = array(Q.out, c(J,niter,K))
out = list(pi=pi.out, Q=Q.out, g=g.out, s=s.out, p=p.out)
class(out) = 'cdmcmc'
out
sample.Q = function(all.a, g, s, Y, alpha,pp){
natt = apply(all.a,1,sum)
cc = tcrossprod(all.a,alpha)
etaim = (cc==natt) # all the attribute patterns smaller than alpha will be true other wise is false
pp[pp<1e-8] = 1e-8
pp[pp>1-1e-8] = 1-1e-8
pp = all.a%*%log(pp) + (1-all.a)%*%log(1-pp)
ga = (1-etaim)%*%Y
gb = (1-etaim)%*%(1-Y)
sa = etaim%*%(1-Y)
sb = etaim%*%Y
pm = ga*log(g) + gb*log(1-g) + sa*log(s) + sb*log(1-s)
pm = pm + pp
pm = pm - max(pm)
pm = as.vector(exp(pm))
pm = pm/sum(pm)
kk = nrow(all.a)
q = sample(1:kk,size=1,prob=pm)
q = (as.binary(c(q,kk))[1,])
q
}
system.time(out <- EstQMCMC(yy, K=4, a.start =1,niter=100))
EstQMCMC = function(Y, K=NULL,q.start = NULL, g.start=NULL, s.start=NULL,pi.start=NULL, a.start,niter){
N = nrow(Y)
J = ncol(Y)
Y = as.matrix(Y)
if(is.null(K) & is.null(q.start))
stop('User must supply either the number of attributes or a starting Q matrix!\n\n')
if(is.null(q.start)){
Q = matrix(rbinom(J*K, 1, 0.5), J, K)
Q[which(apply(Q, 1, sum)==0), 1] = 1
}
else{
Q = q.start
}
if(is.null(g.start))
g = runif(J, 0.1, 0.3)
else
g = g.start
if(is.null(s.start))
s = runif(J, 0.1, 0.3)
else
s = s.start
if(is.null(pi.start)){
pi = exp(rnorm(2^K))
pi = pi/sum(pi)
}
else
pi = pi.start
all.a = as.binary(0:(2^K-1))
natt = apply(Q,1,sum)
Yt = t(Y)
#a <- matrix(NA,nrow = N,ncol = K)
#for (i in 1:N){
#  C <- sample(x = 1:2^K,size = 1,prob = pi)
#  a[i,] = all.a[C,]
#}
a <- a.start
p = ifelse(Q==1, 0.6, 0.4)
pi.out = pi; Q.out = Q; p.out = p; g.out = g; s.out = s
for(ii in 1:niter){
etajm = tcrossprod(Q,all.a) ## number of required attribute mastered for each attribute pattern
natt = apply(Q,1,sum)  ## number of attribute required by each item
etajm = (etajm == natt)    ## whether or not the required pattern fully master the attributes required
pp = g*(1-etajm) + (1-s)*etajm ## probability of each attribute pattern will make the correct response
ll = Y %*% log(pp) + (1-Y)%*%log(1-pp) ## log likelihood of the respone
ll = sweep(ll,2,log(pi),'+') ## add the prior of attribute pattern into the ll
pp = exp(ll) # make it back to the original scale (N * 2^K)
pp = apply(pp,1,cumsum) ## cunsum the proability for each subject over the attribute pattern (2^K * N)
pp = sweep(pp,2,pp[2^K,],'/') ## put the probabilty into the probability scale and ensure the monotocity
## however, in this way the last attribute pattern always has the probability as 1
u = runif(N)
alpha = apply(sweep(pp,2,u,'<'),2,sum)
alpha = as.binary(c(2^K-1,alpha))[-1,]  # generate the attribute profile
cc = as.vector(alpha%*%(2^((K-1):0)))
cc = apply(outer(cc,0:(2^K-1),'=='),2,sum) ## observed proportation of attribute patterns
pi = rgamma(2^K, 1+cc)
pi = pi/sum(pi)
pi.out = rbind(pi.out,pi) # p vector updated!
####### update the slipping and guessing ###########
etaim = tcrossprod(Q,alpha)
etaim = (etaim == natt)
ga = apply((1-etaim)*Yt,1,sum)
gb = apply((1-etaim)*(1-Yt),1,sum)
sa = apply(etaim*(1-Yt),1,sum)
sb = apply(etaim*Yt,1,sum)
g = qbeta(runif(J, 0,pbeta(1-s,1+ga,1+gb)),1+ga,1+gb)
s = qbeta(runif(J,0,pbeta(1-g,1+sa,1+sb)),1+sa,1+sb)
g.out = rbind(g.out,g)
s.out = rbind(s.out,s)
Q = t(sapply(1:J,function(j){sample.Q(all.a[-1,],g[j],s[j],Y[,j],alpha,p[j,])}))
Q.out = rbind(Q.out,Q)
pa = a + Q
pb = a +1-Q
ppp = rbeta(J*K, pa, pb)
p = matrix(ppp, J, K)
p.out = cbind(p.out, p)
}
p.out = array(p.out, c(J,K,niter))
Q.out = array(Q.out, c(J,niter,K))
out = list(pi=pi.out, Q=Q.out, g=g.out, s=s.out, p=p.out)
class(out) = 'cdmcmc'
out
}
sample.Q = function(all.a, g, s, Y, alpha,pp){
natt = apply(all.a,1,sum)
cc = tcrossprod(all.a,alpha)
etaim = (cc==natt) # all the attribute patterns smaller than alpha will be true other wise is false
pp[pp<1e-8] = 1e-8
pp[pp>1-1e-8] = 1-1e-8
pp = all.a%*%log(pp) + (1-all.a)%*%log(1-pp)
ga = (1-etaim)%*%Y
gb = (1-etaim)%*%(1-Y)
sa = etaim%*%(1-Y)
sb = etaim%*%Y
pm = ga*log(g) + gb*log(1-g) + sa*log(s) + sb*log(1-s)
pm = pm + pp
pm = pm - max(pm)
pm = as.vector(exp(pm))
pm = pm/sum(pm)
kk = nrow(all.a)
q = sample(1:kk,size=1,prob=pm)
q = (as.binary(c(q,kk))[1,])
q
}
system.time(out <- EstQMCMC(yy, K=4, a.start =1,niter=100))
library(gtools)
niter = 100
bn = 50
nmb = niter - bn
qest =  apply(out$Q[,-(1:bn),], c(1,3), mean) # delete the burn in iternations
qest
out$Q[,-(1:bn),]
View(out)
out[["Q"]]
temp <- out[["Q"]]
Q.out
dim(qest)
dim(out$Q[,-(1:bn),])
qqq = ifelse(q==1, 0.66, 0.33)
rqest = reorder(J=nrow(q), K=ncol(q), a=qqq, b=qest)
reorder <- function(J, K, a, b){
vec.a = matrix(a, ncol=1)
vec.b = matrix(b, ncol=1)
pm = gtools::permutations(n=K,r=K)
tpm = t(pm)
vec.b.matrix = matrix(as.vector(b[,c(tpm[,1:factorial(K)])]), J*K, factorial(K))
vec.bind.ab = as.matrix(cbind(vec.a, vec.b.matrix))
dist.matrix = as.matrix(dist(t(vec.bind.ab), method="euclidean"))
ds = dist.matrix[,1]
min.value = (min(ds[ds>0]))
matrix.number = as.numeric(which(ds == min.value))
reorder.b = matrix(vec.b.matrix[, matrix.number-1], J, K)
output = list(reorder.b=reorder.b)
output$reorder.b
}
rqest = reorder(J=nrow(q), K=ncol(q), a=qqq, b=qest)
rqest
dim(rqest)
setwd("~/Desktop/yi/professional_study/courses/Multidimnsnl Scaling:Clusterng/HW/HW9")
library(readr)
CSNA_MLT <- read_csv("CSNA_MLT.csv")
View(CSNA_MLT)
knitr::opts_chunk$set(echo = TRUE)
CSNA_MLT <- read_csv("CSNA_MLT.csv")
x <- CSNA_MLT[, 4:17]
x
?kmeans
within.cluster.sum <- c()
for (k in 1:9){
kmean_cluster <- kmeans(x = x,centers = k,iter.max = 100)
within.cluster.sum <- c(within.cluster.sum,sum(keamn$withinss))
}
within.cluster.sum <- c()
for (k in 1:9){
kmean_cluster <- kmeans(x = x,centers = k,iter.max = 100)
within.cluster.sum <- c(within.cluster.sum,sum(kmean_cluster$withinss))
}
within.cluster.sum
within.cluster.sum <- c()
for (k in 1:9){
kmean_cluster <- kmeans(x = x,centers = k,iter.max = 100)
within.cluster.sum <- c(within.cluster.sum,sum(kmean_cluster$withinss))
}
within.cluster.sum
plot(1:9,within.cluster.sum)
plot(1:9,within.cluster.sum,type = 'l')
plot(1:9,within.cluster.sum,type = '-o')
within.cluster.sum <- c()
for (k in 1:9){
kmean_cluster <- kmeans(x = x,centers = k,iter.max = 100)
within.cluster.sum <- c(within.cluster.sum,sum(kmean_cluster$withinss))
}
plot(1:9,within.cluster.sum,type = 'l')
membership <- kmean_cluster$cluster
membership
# rerun the solution
kmean_cluster <- kmeans(x = x,centers = 3,iter.max = 100)
(membership <- kmean_cluster$cluster)
x
which(membership==1)
View(x)
# internal validity
cluster_mean <- matrix(NA, nrow = 14, ncol =3)
cluster_sd <- matrix(NA, nrow = 14, ncol =3)
for (k in 1:3){
for (d in 1:14){
cluster_mean[d,k] <- mean(x[,d][which(membership==k)])
cluster_sd[d,k] <- sd(x[,d][which(membership==k)])
}
}
cluster_mean[d,k] <- mean(x[,d][which(membership==k)])
mean(x[,d][which(membership==k)])
x[,d][which(membership==k)]
cluster_mean[d,k] <- mean(x[which(membership==k),d])
# internal validity
cluster_mean <- matrix(NA, nrow = 14, ncol =3)
cluster_sd <- matrix(NA, nrow = 14, ncol =3)
for (k in 1:3){
for (d in 1:14){
cluster_mean[d,k] <- mean(x[which(membership==k),d])
cluster_sd[d,k] <- sd(x[which(membership==k),d])
}
}
View(cluster_mean)
View(cluster_sd)
cluster_mean[d,k] <- mean(x[which(membership==k),d])
x
x[which(membership==k),d]
x[which(membership==k),d]
mean(x[which(membership==k),d])
mean(as.numeric(x[which(membership==k),d]))
as.matrix(x)
library(readr)
CSNA_MLT <- read_csv("CSNA_MLT.csv")
x <- CSNA_MLT[, 4:17]
x <- as.matrix(x)
library(readr)
CSNA_MLT <- read_csv("CSNA_MLT.csv")
x <- CSNA_MLT[, 4:17]
x <- as.matrix(x)
within.cluster.sum <- c()
for (k in 1:9){
kmean_cluster <- kmeans(x = x,centers = k,iter.max = 100)
within.cluster.sum <- c(within.cluster.sum,sum(kmean_cluster$withinss))
}
plot(1:9,within.cluster.sum,type = 'l')
# rerun the solution
kmean_cluster <- kmeans(x = x,centers = 3,iter.max = 100)
(membership <- kmean_cluster$cluster)
# internal validity
cluster_mean <- matrix(NA, nrow = 14, ncol =3)
cluster_sd <- matrix(NA, nrow = 14, ncol =3)
for (k in 1:3){
for (d in 1:14){
cluster_mean[d,k] <- mean(x[which(membership==k),d])
cluster_sd[d,k] <- sd(x[which(membership==k),d])
}
}
# internal validity
cluster_mean <- matrix(NA, nrow = 14, ncol =3)
cluster_sd <- matrix(NA, nrow = 14, ncol =3)
for (k in 1:3){
for (d in 1:14){
cluster_mean[d,k] <- mean(x[which(membership==k),d])
cluster_sd[d,k] <- sd(x[which(membership==k),d])
}
}
cluster_mean
cluster_sd
# internal validity
cluster_mean <- matrix(NA, nrow = 14, ncol =3)
cluster_sd <- matrix(NA, nrow = 14, ncol =3)
for (k in 1:3){
for (d in 1:14){
cluster_mean[d,k] <- mean(x[which(membership==k),d])
cluster_sd[d,k] <- sd(x[which(membership==k),d])
}
}
colnames(cluster_mean) <- c("cluster 1","cluster 2","cluster 3")
colnames(cluster_sd) <- c("cluster 1","cluster 2","cluster 3")
cluster_mean
cluster_sd
View(cluster_mean)
View(CSNA_MLT)
cluster_spec_mean <- c()
cluster_spec_sd <- c()
for (k in 1:3){
cluster_spec_mean <- c(cluster_spec_mean,mean(CSNA_MLT$spec[which(membership==k)]))
cluster_spec_sd <- c(cluster_spec_mean,sd(CSNA_MLT$spec[which(membership==k)]))
}
cluster_spec_mean <- c()
cluster_spec_sd <- c()
for (k in 1:3){
cluster_spec_mean <- c(cluster_spec_mean,mean(CSNA_MLT$spec[which(membership==k)]))
cluster_spec_sd <- c(cluster_spec_mean,sd(CSNA_MLT$spec[which(membership==k)]))
}
cluster_spec_mean
cluster_spec_sd
cluster_spec_mean <- c()
cluster_spec_sd <- c()
for (k in 1:3){
cluster_spec_mean <- c(cluster_spec_mean,mean(CSNA_MLT$spec[which(membership==k)]))
cluster_spec_sd <- c(cluster_spec_sd,sd(CSNA_MLT$spec[which(membership==k)]))
}
cluster_spec_mean
cluster_spec_sd
cor(CSNA_MLT$i13,CSNA_MLT$spec)
