---
title: "Howework2"
author: "Yi Chen"
date: "1/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Homework 2

### Step 1

Read the data
```{r message=FALSE, warning=FALSE}
library(dplyr)
library(readxl)
options(digits=2)
skills2020 <- read_excel("HUDM 5124_class skills data_2016-2020.xlsx")
# summary(skills2020)
```

Set the skill matrix, replace the NA with 0, and delete the 5th column. There is another column which is not numerical since there is a value in the column is string. I repalce that string value as 1 and transfer the type of column into numberic.


```{r}
sk <- skills2020 %>% 
      replace(., is.na(.), 0) %>%  ## repalce all the missing data as 0
      select(7:(ncol(skills2020)-1)) %>%  ## seiect the skill columns
      mutate(OTHER = replace(OTHER,! OTHER  %in% c('0','1'),'1')) %>%  # replace the OTHER columns
      mutate_if(is.character,as.numeric) %>% ## make all columns as numberic
      select_if(function(col) is.numeric(col) && mean(col)>0) %>%  
      mutate_if(is.numeric, ~1 * (. > 0))  ##repalce all the number bigger than 1 as 1
#summary(sk)
```


Get the correlation matrix (23 time 23 since I keep the column "OTHER")
```{r}
Rxc <- cor(sk)
#round(Rxc,2)
```



## step 2

### A. use princomp to run a PCA
```{r}
princ <- princomp(x = sk,cor = T, scores = T)
print(princ$sdev,digits = 2)
plot(princ$sdev, xlab = "Principal Component",ylab = "Eigenvalue",type = "b", main = 'scree plot1')
abline(h=1,col='Red')
plot(cumsum(princ$sdev)/sum(princ$sdev), xlab = "Principal Component",ylab = "Proportion of Variance Explained",type = "b", main = 'scree plot2')
abline(h=0.8,col='Red')
```

If we only select the principal components which has the standard deviation bigger than 1. Then, the number of principal should be 7. Some people also chooce the number of components which can at least cover 80% of the total variance. In this case, the number of component will be 15.

I personally prefer to chooce a smaller number of principal component since it will realize the popurpose of simplify the model and easier for interpretation. In this case, since almost all principal component covers only a small number of variance, it is hard to really use the PCA to reduce the dimension. I choose to pick 3 principal component, which I recognize that is very risk, since it only capture 24.3% of total variance. I choose 3 mianly because it is easiler to discuss the result and interprete it in the following questions. Another reason is that the decrease of eignvalue from principal 3 to 4 is slower than the decrase from principal 2 to 3. To some extent, this means what the marginal benefit of adding a extra principal tends to be small. Consequently, there is no strong necessary to add more principal.


### B.

```{r}
## print out the first 3 components
p3 <- princ$loadings[,1:3]
print(p3)
```

Note: since I pick 3  components, there are 6 paris.
```{r}
possible_combination <- combn(3,2)
for (c in 1:ncol(possible_combination)){
  combination <- possible_combination[,c]
  cur_pair <- p3[,combination]
  plot(cur_pair,pch = '')
  text(cur_pair,colnames(sk))
}
```

Interpretation: 
Component 1 is SPSS since it is the only positive number. 

Component 2 is traditional software skill which has bigger value for the close-source softwares like Mplus, SPSS, and Stata. It also have big positive number for the math skills like Algebra and Calculus. 

Component 3 is new software skill which has bigger number for new open-source softwares like python and R.




### C.
```{r}
skill.eigen <- eigen(cor(sk))
skill.eigen$values
#round(skill.eigen$vectors,2)
```

Assume that I pick 3 component as what I did in question B
```{r}
# compute the principal component from eignvalue and eigenvector
wgt <-diag(3)
for (i in 1:3){
  wgt[i,i] <- sqrt(skill.eigen$values[i])
}
wgt
```

```{r}
p3_eigen <- skill.eigen$vectors[,1:3] %*% wgt
p3_eigen
```

Compare the result from princomp and eigenvalue eigenvector 
```{r}
# linear corelation
c(cor(p3[,1],p3_eigen[,1]),cor(p3[,2],p3_eigen[,2]),cor(p3[,3],p3_eigen[,3]))

plot(p3[,1],p3_eigen[,1])
abline(lm(p3_eigen[,1]~p3[,1]))

plot(p3[,2],p3_eigen[,2])
abline(lm(p3_eigen[,2]~p3[,2]))

plot(p3[,3],p3_eigen[,3])
abline(lm(p3_eigen[,3]~p3[,3]))

```


Even thouth the result are not the same from eigenvalue/eigenvecotr and princomp. The linear correlation between them are perfect. Which means the result are only differ in linear transformation which means they are fundamentally the same.


rotation rutine
```{r}
p3vm <- varimax(p3_eigen,normalize = T,eps = 1e-5)
p3vm$loadings
```

