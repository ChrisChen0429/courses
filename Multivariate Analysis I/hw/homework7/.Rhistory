#####   Estimation   #####
##########################
as.binary = function(x){
ans = NULL
while(any(x!=0)){
ans = cbind(x%%2,ans)
x = floor(x/2)
}
ans
}
EstQMCMC = function(Y, K=NULL,q.start = NULL, g.start=NULL, s.start=NULL,pi.start=NULL, niter){
N = nrow(Y)
J = ncol(Y)
Y = as.matrix(Y)
if(is.null(K) & is.null(q.start))
stop('User must supply either the number of attributes or a starting Q matrix!\n\n')
if(is.null(q.start)){
Q = matrix(rbinom(J*K, 1, 0.5), J, K)
Q[which(apply(Q, 1, sum)==0), 1] = 1
}
else{
Q = q.start
}
if(is.null(g.start))
g = runif(J, 0.1, 0.3)
else
g = g.start
if(is.null(s.start))
s = runif(J, 0.1, 0.3)
else
s = s.start
if(is.null(pi.start)){
pi = exp(rnorm(2^K))
pi = pi/sum(pi)
}
else
pi = pi.start
all.a = as.binary(0:(2^K-1))
natt = apply(Q,1,sum)
Yt = t(Y)
a <- matrix(NA,nrow = N,ncol = K)
for (i in 1:N){
C <- sample(x = 1:2^K,size = 1,prob = pi)
a[i,] = all.a[C,]
}
p = ifelse(Q==1, 0.6, 0.4)
pi.out = pi; Q.out = Q; p.out = p; g.out = g; s.out = s
for(ii in 1:niter){
etajm = tcrossprod(Q,all.a) ## number of required attribute mastered for each attribute pattern
natt = apply(Q,1,sum)  ## number of attribute required by each item
etajm = (etajm == natt)    ## whether or not the required pattern fully master the attributes required
pp = g*(1-etajm) + (1-s)*etajm ## probability of each attribute pattern will make the correct response
ll = Y %*% log(pp) + (1-Y)%*%log(1-pp) ## log likelihood of the respone
ll = sweep(ll,2,log(pi),'+') ## add the prior of attribute pattern into the ll
pp = exp(ll) # make it back to the original scale (N * 2^K)
pp = apply(pp,1,cumsum) ## cunsum the proability for each subject over the attribute pattern (2^K * N)
pp = sweep(pp,2,pp[2^K,],'/') ## put the probabilty into the probability scale and ensure the monotocity
## however, in this way the last attribute pattern always has the probability as 1
u = runif(N)
alpha = apply(sweep(pp,2,u,'<'),2,sum)
alpha = as.binary(c(2^K-1,alpha))[-1,]  # generate the attribute profile
cc = as.vector(alpha%*%(2^((K-1):0)))
cc = apply(outer(cc,0:(2^K-1),'=='),2,sum) ## observed proportation of attribute patterns
pi = rgamma(2^K, 1+cc)
pi = pi/sum(pi)
pi.out = rbind(pi.out,pi) # p vector updated!
####### update the slipping and guessing ###########
etaim = tcrossprod(Q,alpha)
etaim = (etaim == natt)
ga = apply((1-etaim)*Yt,1,sum)
gb = apply((1-etaim)*(1-Yt),1,sum)
sa = apply(etaim*(1-Yt),1,sum)
sb = apply(etaim*Yt,1,sum)
g = qbeta(runif(J, 0,pbeta(1-s,1+ga,1+gb)),1+ga,1+gb)
s = qbeta(runif(J,0,pbeta(1-g,1+sa,1+sb)),1+sa,1+sb)
g.out = rbind(g.out,g)
s.out = rbind(s.out,s)
Q = t(sapply(1:J,function(j){sample.Q(all.a[-1,],g[j],s[j],Y[,j],alpha,p[j,])}))
Q.out = rbind(Q.out,Q)
pa = a + Q; pb = a +1-Q
ppp = rbeta(J*K, pa, pb)
p = matrix(ppp, J, K)
p.out = cbind(p.out, p)
}
p.out = array(p.out, c(J,K,niter))
Q.out = array(Q.out, c(J,niter,K))
out = list(pi=pi.out, Q=Q.out, g=g.out, s=s.out, p=p.out)
class(out) = 'cdmcmc'
out
}
sample.Q = function(all.a, g, s, Y, alpha,pp){
natt = apply(all.a,1,sum)
cc = tcrossprod(all.a,alpha)
etaim = (cc==natt) # all the attribute patterns smaller than alpha will be true other wise is false
pp[pp<1e-8] = 1e-8
pp[pp>1-1e-8] = 1-1e-8
pp = all.a%*%log(pp) + (1-all.a)%*%log(1-pp)
ga = (1-etaim)%*%Y
gb = (1-etaim)%*%(1-Y)
sa = etaim%*%(1-Y)
sb = etaim%*%Y
pm = ga*log(g) + gb*log(1-g) + sa*log(s) + sb*log(1-s)
pm = pm + pp
pm = pm - max(pm)
pm = as.vector(exp(pm))
pm = pm/sum(pm)
kk = nrow(all.a)
q = sample(1:kk,size=1,prob=pm)
q = (as.binary(c(q,kk))[1,])
q
}
system.time(out <- EstQMCMC(yy, K=4, niter=100000))
system.time(out <- EstQMCMC(yy, K=4, niter=100))
yy
K=4
niter=100
N = nrow(Y)
J = ncol(Y)
Y = as.matrix(Y)
if(is.null(K) & is.null(q.start))
stop('User must supply either the number of attributes or a starting Q matrix!\n\n')
Q = matrix(rbinom(J*K, 1, 0.5), J, K)
Q[which(apply(Q, 1, sum)==0), 1] = 1
g = runif(J, 0.1, 0.3)
s = runif(J, 0.1, 0.3)
pi = exp(rnorm(2^K))
pi = pi/sum(pi)
pi = pi.start
all.a = as.binary(0:(2^K-1))
natt = apply(Q,1,sum)
Yt = t(Y)
a <- matrix(NA,nrow = N,ncol = K)
for (i in 1:N){
C <- sample(x = 1:2^K,size = 1,prob = pi)
a[i,] = all.a[C,]
}
p = ifelse(Q==1, 0.6, 0.4)
pi.out = pi; Q.out = Q; p.out = p; g.out = g; s.out = s
for(ii in 1:niter){
etajm = tcrossprod(Q,all.a) ## number of required attribute mastered for each attribute pattern
natt = apply(Q,1,sum)  ## number of attribute required by each item
etajm = (etajm == natt)    ## whether or not the required pattern fully master the attributes required
pp = g*(1-etajm) + (1-s)*etajm ## probability of each attribute pattern will make the correct response
ll = Y %*% log(pp) + (1-Y)%*%log(1-pp) ## log likelihood of the respone
ll = sweep(ll,2,log(pi),'+') ## add the prior of attribute pattern into the ll
pp = exp(ll) # make it back to the original scale (N * 2^K)
pp = apply(pp,1,cumsum) ## cunsum the proability for each subject over the attribute pattern (2^K * N)
pp = sweep(pp,2,pp[2^K,],'/') ## put the probabilty into the probability scale and ensure the monotocity
## however, in this way the last attribute pattern always has the probability as 1
u = runif(N)
alpha = apply(sweep(pp,2,u,'<'),2,sum)
alpha = as.binary(c(2^K-1,alpha))[-1,]  # generate the attribute profile
cc = as.vector(alpha%*%(2^((K-1):0)))
cc = apply(outer(cc,0:(2^K-1),'=='),2,sum) ## observed proportation of attribute patterns
pi = rgamma(2^K, 1+cc)
pi = pi/sum(pi)
pi.out = rbind(pi.out,pi) # p vector updated!
####### update the slipping and guessing ###########
etaim = tcrossprod(Q,alpha)
etaim = (etaim == natt)
ga = apply((1-etaim)*Yt,1,sum)
gb = apply((1-etaim)*(1-Yt),1,sum)
sa = apply(etaim*(1-Yt),1,sum)
sb = apply(etaim*Yt,1,sum)
g = qbeta(runif(J, 0,pbeta(1-s,1+ga,1+gb)),1+ga,1+gb)
s = qbeta(runif(J,0,pbeta(1-g,1+sa,1+sb)),1+sa,1+sb)
g.out = rbind(g.out,g)
s.out = rbind(s.out,s)
Q = t(sapply(1:J,function(j){sample.Q(all.a[-1,],g[j],s[j],Y[,j],alpha,p[j,])}))
Q.out = rbind(Q.out,Q)
pa = a + Q; pb = a +1-Q
ppp = rbeta(J*K, pa, pb)
p = matrix(ppp, J, K)
p.out = cbind(p.out, p)
}
ii =1
etajm = tcrossprod(Q,all.a) ## number of required attribute mastered for each attribute pattern
etajm
natt = apply(Q,1,sum)  ## number of attribute required by each item
natt
etajm = (etajm == natt)    ## whether or not the required pattern fully master the attributes required
pp = g*(1-etajm) + (1-s)*etajm ## probability of each attribute pattern will make the correct response
ll = Y %*% log(pp) + (1-Y)%*%log(1-pp) ## log likelihood of the respone
ll = sweep(ll,2,log(pi),'+') ## add the prior of attribute pattern into the ll
pp = exp(ll) # make it back to the original scale (N * 2^K)
pp = apply(pp,1,cumsum) ## cunsum the proability for each subject over the attribute pattern (2^K * N)
pp = sweep(pp,2,pp[2^K,],'/') ## put the probabilty into the probability scale and ensure the monotocity
## however, in this way the last attribute pattern always has the probability as 1
u = runif(N)
alpha = apply(sweep(pp,2,u,'<'),2,sum)
alpha = as.binary(c(2^K-1,alpha))[-1,]  # generate the attribute profile
cc = as.vector(alpha%*%(2^((K-1):0)))
cc = apply(outer(cc,0:(2^K-1),'=='),2,sum) ## observed proportation of attribute patterns
pi = rgamma(2^K, 1+cc)
pi = pi/sum(pi)
pi.out = rbind(pi.out,pi) # p vector updated!
####### update the slipping and guessing ###########
etaim = tcrossprod(Q,alpha)
etaim = (etaim == natt)
ga = apply((1-etaim)*Yt,1,sum)
gb = apply((1-etaim)*(1-Yt),1,sum)
sa = apply(etaim*(1-Yt),1,sum)
sb = apply(etaim*Yt,1,sum)
g = qbeta(runif(J, 0,pbeta(1-s,1+ga,1+gb)),1+ga,1+gb)
s = qbeta(runif(J,0,pbeta(1-g,1+sa,1+sb)),1+sa,1+sb)
g.out = rbind(g.out,g)
s.out = rbind(s.out,s)
Q = t(sapply(1:J,function(j){sample.Q(all.a[-1,],g[j],s[j],Y[,j],alpha,p[j,])}))
Q
Q.out = rbind(Q.out,Q)
pa = a + Q; pb = a +1-Q
a
Q
dim(a)
dim(Q)
pa = a + Q
pb = a +1-Q
1-Q
Q
a.start =1
#a <- matrix(NA,nrow = N,ncol = K)
#for (i in 1:N){
#  C <- sample(x = 1:2^K,size = 1,prob = pi)
#  a[i,] = all.a[C,]
#}
a <- a.start
pa = a + Q
pb = a +1-Q
ppp = rbeta(J*K, pa, pb)
p = matrix(ppp, J, K)
p.out = cbind(p.out, p)
for(ii in 1:niter){
etajm = tcrossprod(Q,all.a) ## number of required attribute mastered for each attribute pattern
natt = apply(Q,1,sum)  ## number of attribute required by each item
etajm = (etajm == natt)    ## whether or not the required pattern fully master the attributes required
pp = g*(1-etajm) + (1-s)*etajm ## probability of each attribute pattern will make the correct response
ll = Y %*% log(pp) + (1-Y)%*%log(1-pp) ## log likelihood of the respone
ll = sweep(ll,2,log(pi),'+') ## add the prior of attribute pattern into the ll
pp = exp(ll) # make it back to the original scale (N * 2^K)
pp = apply(pp,1,cumsum) ## cunsum the proability for each subject over the attribute pattern (2^K * N)
pp = sweep(pp,2,pp[2^K,],'/') ## put the probabilty into the probability scale and ensure the monotocity
## however, in this way the last attribute pattern always has the probability as 1
u = runif(N)
alpha = apply(sweep(pp,2,u,'<'),2,sum)
alpha = as.binary(c(2^K-1,alpha))[-1,]  # generate the attribute profile
cc = as.vector(alpha%*%(2^((K-1):0)))
cc = apply(outer(cc,0:(2^K-1),'=='),2,sum) ## observed proportation of attribute patterns
pi = rgamma(2^K, 1+cc)
pi = pi/sum(pi)
pi.out = rbind(pi.out,pi) # p vector updated!
####### update the slipping and guessing ###########
etaim = tcrossprod(Q,alpha)
etaim = (etaim == natt)
ga = apply((1-etaim)*Yt,1,sum)
gb = apply((1-etaim)*(1-Yt),1,sum)
sa = apply(etaim*(1-Yt),1,sum)
sb = apply(etaim*Yt,1,sum)
g = qbeta(runif(J, 0,pbeta(1-s,1+ga,1+gb)),1+ga,1+gb)
s = qbeta(runif(J,0,pbeta(1-g,1+sa,1+sb)),1+sa,1+sb)
g.out = rbind(g.out,g)
s.out = rbind(s.out,s)
Q = t(sapply(1:J,function(j){sample.Q(all.a[-1,],g[j],s[j],Y[,j],alpha,p[j,])}))
Q.out = rbind(Q.out,Q)
pa = a + Q
pb = a +1-Q
ppp = rbeta(J*K, pa, pb)
p = matrix(ppp, J, K)
p.out = cbind(p.out, p)
}
p.out = array(p.out, c(J,K,niter))
Q.out = array(Q.out, c(J,niter,K))
out = list(pi=pi.out, Q=Q.out, g=g.out, s=s.out, p=p.out)
class(out) = 'cdmcmc'
out
sample.Q = function(all.a, g, s, Y, alpha,pp){
natt = apply(all.a,1,sum)
cc = tcrossprod(all.a,alpha)
etaim = (cc==natt) # all the attribute patterns smaller than alpha will be true other wise is false
pp[pp<1e-8] = 1e-8
pp[pp>1-1e-8] = 1-1e-8
pp = all.a%*%log(pp) + (1-all.a)%*%log(1-pp)
ga = (1-etaim)%*%Y
gb = (1-etaim)%*%(1-Y)
sa = etaim%*%(1-Y)
sb = etaim%*%Y
pm = ga*log(g) + gb*log(1-g) + sa*log(s) + sb*log(1-s)
pm = pm + pp
pm = pm - max(pm)
pm = as.vector(exp(pm))
pm = pm/sum(pm)
kk = nrow(all.a)
q = sample(1:kk,size=1,prob=pm)
q = (as.binary(c(q,kk))[1,])
q
}
system.time(out <- EstQMCMC(yy, K=4, a.start =1,niter=100))
EstQMCMC = function(Y, K=NULL,q.start = NULL, g.start=NULL, s.start=NULL,pi.start=NULL, a.start,niter){
N = nrow(Y)
J = ncol(Y)
Y = as.matrix(Y)
if(is.null(K) & is.null(q.start))
stop('User must supply either the number of attributes or a starting Q matrix!\n\n')
if(is.null(q.start)){
Q = matrix(rbinom(J*K, 1, 0.5), J, K)
Q[which(apply(Q, 1, sum)==0), 1] = 1
}
else{
Q = q.start
}
if(is.null(g.start))
g = runif(J, 0.1, 0.3)
else
g = g.start
if(is.null(s.start))
s = runif(J, 0.1, 0.3)
else
s = s.start
if(is.null(pi.start)){
pi = exp(rnorm(2^K))
pi = pi/sum(pi)
}
else
pi = pi.start
all.a = as.binary(0:(2^K-1))
natt = apply(Q,1,sum)
Yt = t(Y)
#a <- matrix(NA,nrow = N,ncol = K)
#for (i in 1:N){
#  C <- sample(x = 1:2^K,size = 1,prob = pi)
#  a[i,] = all.a[C,]
#}
a <- a.start
p = ifelse(Q==1, 0.6, 0.4)
pi.out = pi; Q.out = Q; p.out = p; g.out = g; s.out = s
for(ii in 1:niter){
etajm = tcrossprod(Q,all.a) ## number of required attribute mastered for each attribute pattern
natt = apply(Q,1,sum)  ## number of attribute required by each item
etajm = (etajm == natt)    ## whether or not the required pattern fully master the attributes required
pp = g*(1-etajm) + (1-s)*etajm ## probability of each attribute pattern will make the correct response
ll = Y %*% log(pp) + (1-Y)%*%log(1-pp) ## log likelihood of the respone
ll = sweep(ll,2,log(pi),'+') ## add the prior of attribute pattern into the ll
pp = exp(ll) # make it back to the original scale (N * 2^K)
pp = apply(pp,1,cumsum) ## cunsum the proability for each subject over the attribute pattern (2^K * N)
pp = sweep(pp,2,pp[2^K,],'/') ## put the probabilty into the probability scale and ensure the monotocity
## however, in this way the last attribute pattern always has the probability as 1
u = runif(N)
alpha = apply(sweep(pp,2,u,'<'),2,sum)
alpha = as.binary(c(2^K-1,alpha))[-1,]  # generate the attribute profile
cc = as.vector(alpha%*%(2^((K-1):0)))
cc = apply(outer(cc,0:(2^K-1),'=='),2,sum) ## observed proportation of attribute patterns
pi = rgamma(2^K, 1+cc)
pi = pi/sum(pi)
pi.out = rbind(pi.out,pi) # p vector updated!
####### update the slipping and guessing ###########
etaim = tcrossprod(Q,alpha)
etaim = (etaim == natt)
ga = apply((1-etaim)*Yt,1,sum)
gb = apply((1-etaim)*(1-Yt),1,sum)
sa = apply(etaim*(1-Yt),1,sum)
sb = apply(etaim*Yt,1,sum)
g = qbeta(runif(J, 0,pbeta(1-s,1+ga,1+gb)),1+ga,1+gb)
s = qbeta(runif(J,0,pbeta(1-g,1+sa,1+sb)),1+sa,1+sb)
g.out = rbind(g.out,g)
s.out = rbind(s.out,s)
Q = t(sapply(1:J,function(j){sample.Q(all.a[-1,],g[j],s[j],Y[,j],alpha,p[j,])}))
Q.out = rbind(Q.out,Q)
pa = a + Q
pb = a +1-Q
ppp = rbeta(J*K, pa, pb)
p = matrix(ppp, J, K)
p.out = cbind(p.out, p)
}
p.out = array(p.out, c(J,K,niter))
Q.out = array(Q.out, c(J,niter,K))
out = list(pi=pi.out, Q=Q.out, g=g.out, s=s.out, p=p.out)
class(out) = 'cdmcmc'
out
}
sample.Q = function(all.a, g, s, Y, alpha,pp){
natt = apply(all.a,1,sum)
cc = tcrossprod(all.a,alpha)
etaim = (cc==natt) # all the attribute patterns smaller than alpha will be true other wise is false
pp[pp<1e-8] = 1e-8
pp[pp>1-1e-8] = 1-1e-8
pp = all.a%*%log(pp) + (1-all.a)%*%log(1-pp)
ga = (1-etaim)%*%Y
gb = (1-etaim)%*%(1-Y)
sa = etaim%*%(1-Y)
sb = etaim%*%Y
pm = ga*log(g) + gb*log(1-g) + sa*log(s) + sb*log(1-s)
pm = pm + pp
pm = pm - max(pm)
pm = as.vector(exp(pm))
pm = pm/sum(pm)
kk = nrow(all.a)
q = sample(1:kk,size=1,prob=pm)
q = (as.binary(c(q,kk))[1,])
q
}
system.time(out <- EstQMCMC(yy, K=4, a.start =1,niter=100))
library(gtools)
niter = 100
bn = 50
nmb = niter - bn
qest =  apply(out$Q[,-(1:bn),], c(1,3), mean) # delete the burn in iternations
qest
out$Q[,-(1:bn),]
View(out)
out[["Q"]]
temp <- out[["Q"]]
Q.out
dim(qest)
dim(out$Q[,-(1:bn),])
qqq = ifelse(q==1, 0.66, 0.33)
rqest = reorder(J=nrow(q), K=ncol(q), a=qqq, b=qest)
reorder <- function(J, K, a, b){
vec.a = matrix(a, ncol=1)
vec.b = matrix(b, ncol=1)
pm = gtools::permutations(n=K,r=K)
tpm = t(pm)
vec.b.matrix = matrix(as.vector(b[,c(tpm[,1:factorial(K)])]), J*K, factorial(K))
vec.bind.ab = as.matrix(cbind(vec.a, vec.b.matrix))
dist.matrix = as.matrix(dist(t(vec.bind.ab), method="euclidean"))
ds = dist.matrix[,1]
min.value = (min(ds[ds>0]))
matrix.number = as.numeric(which(ds == min.value))
reorder.b = matrix(vec.b.matrix[, matrix.number-1], J, K)
output = list(reorder.b=reorder.b)
output$reorder.b
}
rqest = reorder(J=nrow(q), K=ncol(q), a=qqq, b=qest)
rqest
dim(rqest)
setwd("~/Desktop/yi/professional_study/courses/Multivariate Analysis I/hw/homework7")
knitr::opts_chunk$set(echo = TRUE)
trackwoman <- read.table("TrackWomen.dat",sep = "\t",header = T)
head(trackwoman,5)
# correlation matrix
R <- cor(trackwoman[,2:ncol(trackwoman)])
round(R,2)
# eigenvalues
(eigenvalue <- eigen(R)$values)
# eigenvector
eigenvector <- eigen(R)$vector
rownames(eigenvector) = colnames(trackwoman[,2:ncol(trackwoman)])
round(eigenvector,2)
# correlation matrix
R <- cor(trackwoman[,2:ncol(trackwoman)])
round(R,2)
# eigenvalues
eigenvalue <- eigen(R)$values
round(eigenvalue,3)
# eigenvector
eigenvector <- eigen(R)$vector
rownames(eigenvector) = colnames(trackwoman[,2:ncol(trackwoman)])
round(eigenvector,2)
# first two principal components
(PC2 <- eigenvector[,1:2])
# the cumulative percentage
sum(eigen(R)$values[1:2])/sum(eigen(R)$values)
loading_1 <-as.matrix(trackwoman[,2:ncol(trackwoman)]) %*% as.matrix(PC2[,1])
rownames(loading_1) <- trackwoman$Country
rownames(loading_1)[rank(-loading_1)]
ncol(trackwoman)
loading_1 <-trackwoman[,2:ncol(trackwoman)] %*% PC2[,1]
)
loading_1 <-as.matrix(trackwoman[,2:ncol(trackwoman)]) %*% as.matrix(PC2[,1])
loading_1
rownames(loading_1) <- trackwoman$Country
loading_1
min(loading_1)
max(loading_1)
loading_1
loading_1 <-as.matrix(trackwoman[,2:ncol(trackwoman)]) %*% as.matrix(PC2[,1])
rownames(loading_1) <- trackwoman$Country
rownames(loading_1)[order(y1[,1],decreasing = T)]
loading_1 <-as.matrix(trackwoman[,2:ncol(trackwoman)]) %*% as.matrix(PC2[,1])
rownames(loading_1) <- trackwoman$Country
rownames(loading_1)[order(loading_1[,1],decreasing = T)]
View(trackwoman)
trackwoman$X100m <- 100/trackwoman$X100m
trackwoman$X200m <- 200/trackwoman$X200m
trackwoman$X400m <- 400/trackwoman$X400m
trackwoman$X800m <- 800/(trackwoman$X800m*60)
trackwoman$X1500m <- 1500/(trackwoman$X1500m*60)
trackwoman$X3000m <- 3000/(trackwoman$X3000m*60)
trackwoman$Marathon <- 42195/(trackwoman$Marathon*60)
head(trackwoman)
R <- cov(trackwoman[,2:ncol(trackwoman)])
round(R,2)
# eigenvalues
(eigenvalue <- eigen(R)$values)
# eigenvector
eigenvector <- eigen(R)$vector
rownames(eigenvector) = colnames(trackwoman[,2:ncol(trackwoman)])
round(eigenvector,2)
R <- cov(trackwoman[,2:ncol(trackwoman)])
round(R,2)
# eigenvalues
eigenvalue <- eigen(R)$values
round(eigenvalue,3)
# eigenvector
eigenvector <- eigen(R)$vector
rownames(eigenvector) = colnames(trackwoman[,2:ncol(trackwoman)])
round(eigenvector,2)
# first two principal components
(PC2 <- eigenvector[,1:2])
# the cumulative percentage
sum(eigen(R)$values[1:2])/sum(eigen(R)$values)
loading_1 <-as.matrix(trackwoman[,2:ncol(trackwoman)]) %*% as.matrix(PC2[,1])
rownames(loading_1) <- trackwoman$Country
rownames(loading_1)[order(loading_1[,1],decreasing = T)]
