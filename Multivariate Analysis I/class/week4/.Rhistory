x <- seq(-5, 5, len= 1000)
plot(x, dnorm(x), type = "l", ylim = c(0, 0.6))
lines(x, dnorm(x, m = -1), col = "red")
lines(x, dnorm(x, m = 1), col = "green")
lines(x, dnorm(x, sd = 1.5), col = "blue")
lines(x, dnorm(x, sd = 0.75), col = "gray54")
legend(-4.5, 0.5, legend=c("N(0,1)", "N(-1,1)", "N(1,0)",
parse(text=paste("N(0, 1.5^2)")), parse(text=paste("N(0, 0.75^2)"))),
col=c("black", "red", "green", "blue", "gray54"), lty = 1, cex=0.8)
parse(text=paste("N(0, 1.5^2)"))
x <- seq(-5, 5, len= 1000)
plot(x, dnorm(x), type = "l", ylim = c(0, 0.6))
lines(x, dnorm(x, m = -1), col = "red")
lines(x, dnorm(x, m = 1), col = "green")
lines(x, dnorm(x, sd = 1.5), col = "blue")
lines(x, dnorm(x, sd = 0.75), col = "gray54")
legend(-4.5, 0.5, legend=c("N(0,1)", "N(-1,1)", "N(1,0)",
"N(0, 1.5^2)","N(0, 0.75^2)"),
col=c("black", "red", "green", "blue", "gray54"), lty = 1, cex=0.8)
# Density curves of various chi-square distributions:
x <- seq(0, 75, len= 500)
plot(x, dchisq(x, 4), type = "l", ylim = c(0, 0.2))
lines(x, dchisq(x, 10), col = "purple")
lines(x, dchisq(x, 20), col = "green")
lines(x, dchisq(x, 30), col = "blue")
lines(x, dchisq(x, 50), col = "gray54")
legend(50, 0.17, legend=c("df = 4", "df = 10", "df = 20", "df = 30", "df = 50"),
col=c("black", "purple", "green", "blue", "gray54"), lty = 1, cex=0.8)
x <- seq(-5, 5, len= 200)
plot(x, dnorm(x), type = "l", ylim = c(0, 0.5))
lines(x, dt(x, 3), col = "purple")
lines(x, dt(x, 11), col = "green")
lines(x, dt(x, 24), col = "blue")
legend(-4, 0.4, legend=c("N(0, 1)", "df = 3", "df = 11", "df = 24"),
col=c("black", "purple", "green", "blue"), lty = 1, cex=0.8)
x <- seq(0, 10, len= 100)
plot(x, df(x, 10, 20), type = "l", ylim = c(0, 0.9))
lines(x, df(x, 5, 10), col = "purple")
lines(x, df(x, 5, 5), col = "yellow")
legend(6, 0.8, legend=c("df = (10, 20)", "df = (5, 10)", "df = (5, 5)"),
col=c("black", "purple", "yellow"), lty = 1, cex=0.8)
# Bivariate normal
# Create the grid first
x <- seq(-3, 3, len = 100)
y <- seq(-3, 3, len = 100)
# Standard bivariate normal
z1 <- (1/(2*pi))*exp(-outer(x^2, y^2, "+")/2)
# Save old paramaters
p = par()
# Make plot area square
par(pty = "s")
contour(x,y,z1)
image(x,y,z1)
persp(x, y, z1, theta = 30, phi = 30, expand = 0.5, col = "lightblue")
x
y
outer(x^2,y^2)
# Save old paramaters
p = par()
# Make plot area square
par(pty = "s")
contour(x,y,z1)
image(x,y,z1)
# Or with package rgl
install.packages("rgl")
library("rgl")
persp3d(x,y,z1)
# Any bivariate normal (notice it is much slower because of the two loops)
x <- seq(-4, 4, len = 100)
y <- seq(-4, 4, len = 100)
#This is the covariance matrix. Make sure it has a positive determinant!
S <- matrix(c(11, 4, 4, 2), 2, 2)
z2 <- matrix(rep(0, 100^2), 100, 100)
d <- det(S)
for (i in 1:100)
for (j in 1:100)
z2[i, j] <- (1/(2*pi*sqrt(d)))*exp(-(t(c(x[i], y[j]))%*%solve(S)%*%c(x[i], y[j]))/2)
contour(x,y,z2)
persp(x,y,z2, theta = 30, phi = 30, expand = 0.5, col = "lightblue")
z2
library(MASS)
# Simulate bivariate normal data
mu <- c(0,0)                         # Mean
(Sigma <- matrix(c(1, .5, .5, 1), 2))  # Covariance matrix
# Generate sample from N(mu, Sigma)
bivn <- mvrnorm(5000, mu = mu, Sigma = Sigma )  # from Mass package
head(bivn)
# Calculate kernel density estimate
bivn.kde <- kde2d(bivn[,1], bivn[,2], n = 50)
image(bivn.kde)       # from base graphics package
contour(bivn.kde, add = TRUE)     # from base graphics package
mu0 <- c(70, 170)
x.bar = c(71.45, 164.7)
# Assume population covariance matrix is given:
S <- matrix(c(20, 100, 100, 1000), 2, 2)
s
S
# Test statistic
z.obs <- 20*t(x.bar - mu0)%*%solve(S)%*%(x.bar - mu0)
z.obs
# Critical chi-sq
qchisq(0.95,df=2)
1-pchisq(z.obs,df=2)
# Doing same test one mu at a time;
(z.obs1 <- 20*(x.bar[1]-mu0[1])^2/S[1,1])
(z.obs2 <- 20*(x.bar[2]-mu0[2])^2/S[2,2])
qchisq(0.95,df=1)
# Sample size n
n = 19
# Let's simulate some data:
# Set the seed if you want to have the same exact numbers as here:
set.seed(874)
(x = rnorm(n, m = 9, sd = 0.5))
hist(x)
# Suppose we want to test H0: mu = 10 vs. mu not equal 10 (that is, two-tail test)
# at alpha level of 0.05
t.test(x, m = 10)
# How about left-tail alternative H1: mu < 10
t.test(x, m= 10, alt = "less")
# Manual computation of the test statistic:
(t.obs = (mean(x) - 10)/(sd(x)/sqrt(n)))
# How about manual p-value:
# Left-tail:
pt(t.obs, n-1)
# Two-tail:
2*(1-pt(abs(t.obs), n-1))
setwd("~/Desktop/yi/Research/AERI/FEAR")
library(readxl)
library(dplyr)
library(tidyr)
library(psy)
library(ggplot2)
library(plyr)
library(stringr)
## backgroud analysis
background <- read_excel("data/background.xlsx")
background <- background %>% slice(c(-1,-2))
## frequency summary
table(background$Q53)
round(table(background$Q53) / sum(table(background$Q53))*100+1,2)
table(background$Q54)
round(table(background$Q54) / sum(table(background$Q54))*100+1,2)
table(background$Q55)
round(table(background$Q55) / sum(table(background$Q55))*100+1,2)
table(background$Q59)
round(table(background$Q59) / sum(table(background$Q59))*100+1,2)
table(background$Q60)
round(table(background$Q60) / sum(table(background$Q60))*100+1,2)
table(background$Q61)
round(table(background$Q61) / sum(table(background$Q61))*100+1,2)
table(background$Q62)
round(table(background$Q62) / sum(table(background$Q62))*100+1,2)
table(background$Q63)
round(table(background$Q63) / sum(table(background$Q63))*100+1,2)
## item analysis
item_response <- read_excel("data/FAER Cleaned DATASet_OB+Anes_MCQ+Validation+Study_Feb_5.xlsx", sheet = "PARALLEL ITEMS")
item_response <- item_response %>% drop_na()
item_response <- item_response %>% dplyr::select(.,-c(SC0,`#KEY`))
key <- item_response[2,]
item_response <- item_response %>% dplyr::slice(c(-1,-2))
domain <- c("CU","CU","HO","HO","CU","CU","App","HO","HO","HO","App","App",
"HO","HO","HO","HO","CU","CU","CU","CU","CU","CU","CU","CU",
"App","App","App","App","App","App","HO","HO","CU","CU","CU","CU",
"App","App","App","App","App","App","HO","HO","HO","HO","HO","HO",
"App","App","App","App")
sub_contruct <- c(rep("PCP",8),rep("PHA",14),rep("AIP",18),rep("AIP",12))
exam_a <- matrix(NA,nrow = 48,ncol = 26)
exam_b <- matrix(NA,nrow = 48,ncol = 26)
for (i in 1:48){
for (j in 1:52){
right_answer <- as.numeric(key[j])
if (j%%2==1){
exam_a[i,floor(j/2)+1] <- as.numeric(item_response[i,j] == right_answer)
}else{
exam_b[i,j/2] <- as.numeric(item_response[i,j] == right_answer)
}
}
}
for (i in 1:48){
for (j in 1:52){
right_answer <- as.numeric(key[j])
item_response[i,j] <- as.numeric(item_response[i,j] == right_answer)
}
}
examinee_index <- rep(1:48,52)
long_data <- data.frame(examinee_index=examinee_index,
domian=rep(domain,each=48),
sub_contruct=rep(sub_contruct,each=48),
AB_index = rep(rep(c("A","B"),26),each=48),
item_index = rep(colnames(item_response),each=48),
score = as.numeric(as.matrix(item_response)))
View(long_data)
long_data <- long_data%>%dplyr::mutate(item_index = str_extract(item_index, "Q[0-9]+"))
frequency <- long_data %>% group_by(domian,sub_contruct,AB_index) %>% dplyr::summarise(count = n()/48)
View(long_data)
setwd("~/Desktop/yi/professional_study/courses/Multivariate Analysis I/class/week4")
# Read the dataset first
x <- read.table("T5_1_PSYCH.DAT")
x
# Read the dataset first
x <- read.table("T5_1_PSYCH.DAT")
View(x)
# V1 = 1 means male
# Partitioning the date into two groups (males and females)
X1 <- as.matrix(x[1:32,2:5])
X2 <- as.matrix(x[33:64,2:5])
View(X1)
View(X2)
x1.bar <- apply(X1, 2, mean)
x2.bar <- apply(X2, 2, mean)
S1 <- cov(X1)
S2 <- cov(X2)
n1 = nrow(X1)
n2 = nrow(X2)
Sp <- (1/(n1 + n2 -2))*((n1-1)*S1 + (n2-1)*S2)
Sp
T.sq.test <- (n1*n2/(n1+n2))*t(x1.bar-x2.bar) %*% solve(Sp) %*% (x1.bar-x2.bar)
T.sq.test
a <- solve(Sp)%*%(x1.bar - x2.bar)
a
# Scaled version of Hotelling's T^2 that has F distribution
p <- ncol(X1)
((n1+n2-p-1)/((n1+n2-2)*p))*T.sq.test
# Compare to F(p, n1+n2-p-1) distribution
# p-value
pf(((n1+n2-p-1)/((n1+n2-2)*p))*T.sq.test, p ,n1+n2-p-1,lower.tail=FALSE)
# 95% CI for mu1-mu2
qf(0.95,p, n1 + n2 - p -1)
(c.sq = (n1 + n2 - 2)*p/(n1+ n2 -p - 1)*qf(0.95,p, n1 + n2 - p -1))
# mu11 - mu21:
(x1.bar[1] - x2.bar[1]) - sqrt(c.sq)*sqrt((1/n1 + 1/n2)*Sp[1,1])
(x1.bar[1] - x2.bar[1]) + sqrt(c.sq)*sqrt((1/n1 + 1/n2)*Sp[1,1])
x <- read.table("T6-1.DAT")
x
(d = cbind(x$V1 - x$V3, x$V2 - x$V4))
