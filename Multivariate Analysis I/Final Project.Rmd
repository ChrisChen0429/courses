---
title: "Multivariate Analysis 1 Final Project"
author: "Yi Chen"
date: "5/1/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introdcution

This project will show the example of using R to carry out some simple multivariate analyses, with a focus on principal component analysis (PCA) and linear discriminate analysis (LDA). Whole project will guide the reader from the exploratory data analysis to more advanced statistical methodologies. For every technique covered in this project, the corresponding research question, method theory (including assumption and limitation), and R code will be provided. 

## Data Set
The data sets in this project come from the UCI Machine Learning Repository (http://archive.ics.uci.edu/ml). In particular, the wine recognition data (http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)  is updated at Sept 21, 1998 by C.Blake. The original source of the data set comes from Forina, M. et al (1991).

The observations in the data set are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.

```{r}
## load the data

wine <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data",sep=",")
colnames(wine) <- c("type",	"Alcohol","Malic acid","Ash","Alcalinity","Magnesium","Phenols","Flavanoids","Nonflavanoid phenols","Proanthocyanins","Color intensity","Hue","OD280/OD315","Proline")
head(wine,5)
dim(wine)
summary(wine)
```

As we can see, the first column `type` indicates which wine type the sample belongs to. There are 178 samples with 13 distinct attributes. All 13 attributes are continuous variables. There is no missing value in the data set.


## EDA through Visualization 

#### Matrix Scatter Plot

In multivariate analysis, a common way to understand the structure of the data set through visulization is `matrix scatterplot`. For simplicity, I only use the first 5 attribution in the plot.
```{r message=FALSE, warning=FALSE}
library(car)
scatterplotMatrix(wine[2:6])
```

In this matrix scatterplot, the diagonal cells show histograms of each of the variables, in this case the concentrations of the first five chemicals. Each of the off-diagonal cells is a scatterplot of two of the five chemicals. As we can see, there is a certain level of correlation between the pair of the attributes. Meanwhile, the distribution of most attributes approximately follow the normal distribution. But the distribution of Alcohol and Malic.acid is more different from the normal distribution.


#### Scatter Plot with group label.

Using the scatter plot give us a more detail information about how two attribute are correlated with each other. In particular, I label each simple points with their corresponding wine type. In this way, we can see who the data allocate in different two dimensional space across different wine types. I choose the attribute of `alcohol` and `malic acid` as an example. 

```{r}
plot(wine$Alcohol, wine$`Malic acid`)
text(wine$Alcohol, wine$`Malic acid`, wine$type, cex=0.7, pos=4, col="red")
```

In general, there is already a pattern in classification. The second type of wine are located at the range with lower value of alcohol. For the first and third type of wine, they are located at the space with high alcohol but low malic acid and high malic acid. However, there are many points from different types of wine overlap with each other. This indicates that there is a limiation in using just two dimension space for classifying the wine type.


## Discriptive Analysis

#### mean and standard deviation of each attribute
In order to have a deeper understanding of the data set. The first thing we need to know is the basic discriptive statistics for each attribution.
```{r}
round(sapply(wine[2:14], mean),2)
round(sapply(wine[2:14], sd),2)
```

Clearly, we can see there is a big difference in the scale of 13 attribute. This indicates that there is a need for normalize the data before the analysis. Meanwhile, scale the data also reshape the distribution of the data so that it is closer to the normal distribution, which is the assumption for many multivariate analysis models.


#### mean and standard deviation difference across wine type

Secondly, we may also interested in explore the difference of mean and standard deviation across different groups. 
```{r}
printMeanAndSdByGroup <- function(variables,groupvariable)
  {
     # find the names of the variables
     variablenames <- c(names(groupvariable),names(as.data.frame(variables)))
     # within each group, find the mean of each variable
     groupvariable <- groupvariable[,1] # ensures groupvariable is not a list
     means <- aggregate(as.matrix(variables) ~ groupvariable, FUN = mean)
     names(means) <- variablenames
     print(paste("Means:"))
     print(means)
     # within each group, find the standard deviation of each variable:
     sds <- aggregate(as.matrix(variables) ~ groupvariable, FUN = sd)
     names(sds) <- variablenames
     print(paste("Standard deviations:"))
     print(sds)
}

printMeanAndSdByGroup(wine[2:14],wine[1])
```


As an example, I will compare the mean difference between the first and second wine type. There is small mean different between the first and second wine type in terms of hue and ash, but big difference in terms of profline and color density.
This indicates that different attribute has different power in distinguish the wine type. For example, ash and nonflavanoid phenols are simiar across different types of wine. But flavanoids is much lower in the thrid type of wine than the other two type.


#### Between-groups Variance and Within-groups Variance for a Variable

Further, we can calculate the within group variance, between group variance, and Separation of each variable. Similar to the idea in ANOVA, these information help us to identify which variable is more useful in classify the wine type and which varible is more consistant across different wine type.

The within group variance can be calculated using the formula: 
$$\frac{\sum (y_{i1} - \bar{y_1})^2 +...+\sum (y_{g1} - \bar{y_g})^2}{n-g}$$
, where g is the number of group, n is the number of total sample.

```{r}
WithinGroupsVariance <- function(variable,groupvariable){
     n_total <- length(variable)
     n_group <- length(unique(groupvariable))
     total_variance <- 0
     for (g in 1:n_group){
       group_data <- variable[groupvariable==g]
       group_mean <- mean(group_data)
       for (i in group_data){
         total_variance <- total_variance + (i - group_mean)^2
       }
     }
     return(total_variance/(n_total-n_group))
}
WithinGroupsVariance(wine$Alcohol,wine$type)
```

As the example, the within group variande for `alcohol` is about 0.26. Simiarly, we can calculate the between group variance using the formula below,
$$\frac{n_1(\bar{y_1}-\bar{y})^2 + ...+n_g(\bar{y_g}-\bar{y})^2}{g-1}$$
, where $n_i$ represent the number of sample in group $i$ and $\bar{y_i}$ is the group mean.
```{r}
BetweenGroupsVariance <- function(variable,groupvariable){
     n_group <- length(unique(groupvariable))
     grand_mean <- mean(variable)
     total_variance <- 0
     for (g in 1:n_group){
         group_data <- variable[groupvariable==g]
         group_mean <- mean(group_data)
         ng <- length(group_data)
         total_variance <- total_variance + ng*(group_mean - grand_mean)^2
     }
     return(total_variance/(n_group-1))
}
BetweenGroupsVariance(wine$Alcohol,wine$type)
```
As the example, the within group variande for `alcohol` is about 35.39.
We can calculate the “separation” achieved by a variable as its between-groups variance devided by its within-groups variance.
```{r}
35.39742/0.2620525
```
The separation indicate that much more variance in the data set can be explianed by the group difference than the within group uncertainty. A high value of separation means the variboe is useful in classfication since the variance difference across group is significant. Similarly, we can calculate all variables in the data set.

```{r}
for (i in 2:14){
  between <- BetweenGroupsVariance(wine[,i],wine$type)
  within <- WithinGroupsVariance(wine[,i],wine$type)
  variablename <- colnames(wine)[i]
  sep <- between / within
  print(paste("variable",variablename,",Wv=",within,",Bv=",between,",Sep=",sep))
}
```
 
As we can see from the analysis, the variable `Flavanoids` and `Proline` have the high separation, while `Magnesium` and `Nonflavanoid phenols` have the low separation.

## Principal Component Analysis

#### Summary of PCA
Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and patterns. In other word, PCA is a technique for feature extraction — so it combines input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variable. It reduce the high dimension of data space into a lower dimension principal component space through orthogonal linear transformation. The new coordinate system created by the principaal component has greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

The biggest assunmption of PCA is the linear assumption. PCA can find the orthogonal projections of the dataset that contains the highest variance if the data set is linear correlated. Otherwise, it cannot help to reduce the dimension.
In other word,  the underlying structure of the data must be linear, patterns that are highly correlated may be unresolved because all PCs are uncorrelated, and the goal is to maximize variance and not necessarily to find clusters. 
Second, PCA rely on the orthogonal tranformations. Howeever, it may not be the best data transformation in terms of extracting the feature. Third, PCA is scale variant. This means that the scale of the data set will change the result of the PCA. Finally, PCA assume no missing value in the data set.

#### PCA Data Analysis

To carry out a principal component analysis (PCA) on a multivariate data set, the first step is often to standardise the variables. This is necessary if the input variables have very different variances, which is true in this case as the concentrations of the 13 chemicals have very different variances (see above).

```{r}
scale_imput <-scale(wine[,2:14])
wine.pca <- prcomp(scale_imput)   
summary(wine.pca)

screeplot(wine.pca, type="lines")
```

The most obvious change in slope in the scree plot occurs at component four, which is the “elbow” of the scree plot. Therefore, it cound be argued based on the basis of the scree plot that the first three components should be retained. 

```{r}
cumsum((wine.pca$sdev)^2)/sum((wine.pca$sdev)^2)
```

As we can see the first four principal component keeps about 73.59% variance.
There are some other rules to we can pick to determine the number of principal component to keep.
For example, if we need to ensure that at least 80% of the total variance should be maintained. Thus, first five principal components should be picked. 
Another rule is to pick the principal component which has the variance bigger than 1. In this way, we have to keep the first three principal component.
```{r}
(wine.pca$sdev)^2
```

Take the first principal's loading is shown as below.
```{r}
round(wine.pca$rotation[,1],2)
```
This indicates that the first principal component rely more on `Flavanoids`, `Phenols`, and `OD280/OD315`. The variable `Ash` almost contribute zero to the first principal component.
The loading of the principal component is helpful in interpretating the possible meaning of the principal component. In this study, I am not familiar with chemical knowledge of these attributes. So, I will not take the risk to summary the meaning of the principal component.

Finllay, scatterplot of the first two principal components, and label the data points with the cultivar that the wine samples come from will be take as an example. 

```{r}
plot(wine.pca$x[,1],wine.pca$x[,2]) # make a scatterplot
text(wine.pca$x[,1],wine.pca$x[,2], wine$type, cex=0.7, pos=4, col="red") 
```

The scatterplot shows the first principal component on the x-axis, and the second principal component on the y-axis. We can see from the scatterplot that wine samples of cultivar 1 have much lower values of the first principal component than wine samples of cultivar 3. Therefore, the first principal component separates wine samples of cultivars 1 from those of cultivar 3.

We can also see that wine samples of cultivar 2 have much higher values of the second principal component than wine samples of cultivars 1 and 3. Therefore, the second principal component separates samples of cultivar 2 from samples of cultivars 1 and 3.

Therefore, the first two principal components are reasonably useful for distinguishing wine samples of the three different cultivars.

## Linear Discriminate Analysis

#### Summary of LDA

Linear discriminate analysis (LDA) is a method to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.

LDA assume that the approaches the problem by assuming that the conditional probability density functions from both classes are both normally distributed with differnt mean vector and the same variance-covariance matrix (can also be different). If the varianes are assumed to be the same, it is called homoscedasticity assumption. Besides, LDA also take the multicollinearity and indepdence assumption. Predictive power can decrease with an increased correlation between predictor variables. Meanwhile, simples are assumed to be randomly sampled, and a participant's score on one variable is assumed to be independent of scores on that variable for all other participants. Thus, if the data does not follow the multivariate normal distribution, the variables have high multicollinearlity or the samples are not independent, the result of LDA will be biased. Besides, the LDA also rely on the complete data set and binary class.

#### LDA analysis

If we want to separate the wines by cultivar, the wines come from three different cultivars, so the number of groups is 3, and the number of variables is four principal component I calculated in the last section. The maximum number of useful discriminant functions that can separate the wines by cultivar is the minimum of 3-1 and 4, and so in this case it is the minimum of 2 and 13, which is 2. 
The variance is assumed to be the same.

```{r}
library(MASS)
wine.lda <- lda(x = wine.pca$x[,1:4], grouping = wine$type)
plot(wine.lda)
```

As we can see that the two discriminant functions seperate the three wine type well. To see the loading of the discrimiant functions, we can:
```{r}
wine.lda$scaling
```
As we can see, the first discrimiant function rely more on the first principal component, while the second discriminant function rely more on the second principal component.

```{r}
table(Predicted=predict(wine.lda)$class, Actual=wine$type)
```
As we can see from the table above, most of the data are predicted correctly. Only 2 points belongs to the range 3 are predicted to be in range 2. The overall aggrement rate is $1-2/178=98.88%$.

To futher analysis the separation achieved by each discriminant function. Stacked histogram can be used.

```{r}
wine.pred <- predict(wine.lda, wine.pca$x[,1:4])
ldahist(data = wine.pred$x[,1], g=wine$type)  
ldahist(data = wine.pred$x[,2], g=wine$type)  
```

As we can see, the frist discriminant function has the power to distinguish the type 1 and 2 with type 3. However, the difference between 1 and 2 are limiated. The differences between the 3 wine types are more clear seperated in the second discriminant function.


## Reference 
Forina, M. et al. 1991. PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, *Via Brigata Salerno*, 16147 Genoa, Italy.
