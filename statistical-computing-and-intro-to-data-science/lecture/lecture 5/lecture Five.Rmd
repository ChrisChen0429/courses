---
title: "lecture five"
author: "Yi (Chris) Chen"
date: "October 6, 2017"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## lecture five
#### Writing Functions In R     
* the priciple of writing function:     
1. be fairly short
2. focus and specific in what it dose
3. designed so that it can be used in many ways   
   
* basic structure:
function_name <- function(arg1,arg2,...){
        statements
        return(object)
}   

##### example1 of function
```{r}
# A Function to Check for Significance at ?? = 0:05
significant <- function(x){
        if(x <= 0.05){return(TRUE)}
        else{return(FALSE)}
}
significant(1)
significant(0.001)
```

##### example2 of funciton
```{r}
# A `Robust' Loss Function (for Outlier-Resistant Regression)

res_loss <- function(x) {
        loss_vec <- ifelse(x^2 > 1, 2*abs(x) - 1, x^2)
        return(loss_vec)
}

vec <- c(-0.5, 0.9, -3, 4)
res_loss(vec)
```

##### task of function   
Write a function called FiveTimesSum that takes as input a vector of numerical values and returns 5 times the sum of those values. Test it on the vector 1:3. Your output should be 30.

```{r}
FiveTimesSum_1 <- function(x){
        result <- sum(5*(x))
        return(result)
}
vec <- 1:3
FiveTimesSum_1(vec)

# this can also be solved like this
FiveTimesSum_2 <- function(x){
        for(i in 1:length(x)){
                x[i] = x[i] *5
        }
        result <- sum(x)
        return(result)
}
vec <- 1:3
FiveTimesSum_2(vec)
```

##### Named and Default Arguments
```{r}
# Inputs: A vector of numbers (x)
#         crossover location (c>0)
# Outputs: A loss vector with x^2 for small elements(which is smaller than c),
#          and 2|x|-c for large ones(which is bigger than c)
res_loss2 <- function(x, c = 1) {
        loss_vec <- ifelse(x^2 > c, 2*c*abs(x) - c, x^2)
        return(loss_vec)
}

identical(res_loss(vec), res_loss2(vec, c = 1))
identical(res_loss(vec), res_loss2(vec, c = 2))
identical(res_loss2(vec, c = 1), res_loss2(vec))
identical(res_loss2(x = vec, c = 2),res_loss2(c = 2, x = vec))


vec <- c(-0.5, 0.9, -3, 4)
res_loss2(vec, c = c(1,1,1,5)) # they input the value of c respectively. Actually c only make sense when it is a single positive number
res_loss2(vec, c = -1) # even if c can not be a negative number but the function could still work
``` 

* if you want to add some check in your function about the range of c, you can do in this way
```{r}
res_loss3<- function(x,c=1) {
        # Scale should be a single positive number
        stopifnot(length(c)==1,c>0) # stopifnot if the situdation can not be satisfied.
        loss_vec <- ifelse(x^2 > c, 2*abs(x) - c, x^2)
        return(loss_ve2)
}
```

#### task
Write a function the will take an input vector and set any value below a threshold to be the value of the threshold. Optionally, the function should instead set values above the threshold to the value of the threshold. Hint: The function should have three arguments, two required the vector and the threshold, and one optional with a default value of "below".

```{r}
threshold_function <- function(vec,thresh,dir='below'){
        stopifnot(dir == 'below' | dir == 'above')
        if(dir=='below') {
                vec[vec<thresh] <- thresh
        }else{
                vec[vec>thresh] <- thresh
        }
        return(vec)
}
vec <- 1:10
threshold_function(vec,thresh = 5,dir = 'below')
threshold_function(vec,thresh = 5,dir = 'above')
```

### Use Your Functions in Other Functions
```{r}
# automatically try the different value in the function
curve(res_loss2,from = -5 , to= 5)
```

#### grobel environment
```{r}
x <- 2
y <- function(y){
        return(x + y)
}
y(1)  # there is no value of x inside the function so that it will find the value of x in the golbel enviornment

g <- function(y){
        x <-10
        return(x+y)
}
g(1) # it has the value of x in the function so it will not use the value in the gobel environment

g<- function(y){
        f <- function(y){ # the f function is not in the gobel enivorment
                return(x+y)
        }
        x<-10   # x is not in the gobel environment
        return(f(y))
}
g(1)
```

#### everything in R is a function and everything in R is an object
```{r}
'+'(3,2)
class(1)
```

##### extended example: fitting a model
```{r}
setwd("C:/Users/cheny/Desktop/study/statistical computing and intro to data science/lecture five")
gmp <- read.table('gmp.txt',as.is = TRUE,header = TRUE) # as.is means we want or not to make the character as factor
head(gmp)[1:3,]

gmp$pop <- gmp$gmp/gmp$pcgmp
head(gmp,5)

plot(gmp$pop,gmp$pcgmp,log = 'x',xlab='population',ylab = 'per-captita economic ouput')

# beta_0 =6611,beta_1=1/8
curve(6611*x^{1/8},add = TRUE,col="blue")
```

##### fitting the function
* strategy:   
minimize the mean sum of squares of training data
* assume:   
that it is a convex function: twice differentiable function is always bigger than 0 and there is only one minimum (both local and global)
* algorith:   
gradient descent search for a minimum of f
1. start with some point x and fix a precision b bigger than 0   
2. repeat for n= 1,2,3...  that x(n+1)=x(n)- c * f'(x(n)), where c is the scale of step size
3. terminate when |f'(x(n))| < b

##### first attemp at code
```{r}

max.iter <- 100 # how long we run the alg
stop.deriv <- 1/100 # if derivative is samll, stop (b)
deriv.step <- 1/1000 # that is the samll h which is used to calculate the differente
step.scale <- 1e-12 # scale of step size (c)

iter <- 0 # iteration counter
deriv <- Inf
beta <- 0.15 # start point 

while((iter < max.iter) & (deriv > stop.deriv)) {
        # the function will stop if the iteration is too long or the deriv is samller than the b
        iter <- iter + 1
        # calculate the differente
        mse.1 <- mean((gmp$pcgmp - 6611*gmp$pop^beta)^2)
        mse.2 <- mean((gmp$pcgmp - 6611*gmp$pop^(beta + deriv.step))^2)
        deriv <- (mse.2 - mse.1)/deriv.step
        # update the value of beta
        beta <- beta - step.scale*deriv
}
list(beta = beta, iteration = iter, conv = (iter < max.iter))
```

##### first fix
problems:
1. not encapsulated
2. inflexiable
3. error-pone
4. hard to fix
```{r}
est.exp <- function(beta) {
        max.iter <- 100 # how long we run the alg
        stop.deriv <- 1/100 # if derivative is samll, stop (b)     
        deriv.step <- 1/1000 # that is the samll h which is used to calculate the differente
        step.scale <- 1e-12 # scale of step size (c)
        iter <- 0 # iteration counter
        deriv <- Inf

        while((iter < max.iter) & (deriv > stop.deriv)) {
        # the function will stop if the iteration is too long or the deriv is samller than the b
        iter <- iter + 1
        # calculate the differente
        mse.1 <- mean((gmp$pcgmp - 6611*gmp$pop^beta)^2)
        mse.2 <- mean((gmp$pcgmp - 6611*gmp$pop^(beta + deriv.step))^2)
        deriv <- (mse.2 - mse.1)/deriv.step
        # update the value of beta
        beta <- beta - step.scale*deriv
        }
        fit <- list(beta = beta, iteration = iter, conv = (iter < max.iter))
        return(fit)
}
        
est.exp(0.15)
```

###### second fix
problem:
1. have to return if we want to change defined pararmeters
```{r}
est.exp <- function(beta, beta_0 = 6611, max.iter = 100,
stop.deriv = .01, deriv.step = .001,
step.scale = 1e-12) {

        iter <- 0 # iteration counter
        deriv <- Inf

        while((iter < max.iter) & (deriv > stop.deriv)) {
        # the function will stop if the iteration is too long or the deriv is samller than the b
        iter <- iter + 1
        # calculate the differente
        mse.1 <- mean((gmp$pcgmp - 6611*gmp$pop^beta)^2)
        mse.2 <- mean((gmp$pcgmp - 6611*gmp$pop^(beta + deriv.step))^2)
        deriv <- (mse.2 - mse.1)/deriv.step
        # update the value of beta
        beta <- beta - step.scale*deriv
        }
        fit <- list(beta = beta, iteration = iter, conv = (iter < max.iter))
        return(fit)
}
        
est.exp(0.15)

```

###### thrid fix
problem:
1. Problem: Don't need to write out the MSE calculations twice in thebody of the function.
```{r}
est.exp <- function(beta, beta_0 = 6611, max.iter = 100,
stop.deriv = .01, deriv.step = .001,
step.scale = 1e-12) {

        iter <- 0 # iteration counter
        deriv <- Inf

        ## add a function
        mse <- function(b) {mean((gmp$pcgmp - beta_0*gmp$pop^b)^2)}

        
        while((iter < max.iter) & (deriv > stop.deriv)) {
        # the function will stop if the iteration is too long or the deriv is samller than the b
        iter <- iter + 1
        # calculate the differente
        deriv <- (mse(beta + deriv.step) - mse(beta))/deriv.step
        # update the value of beta
        beta <- beta - step.scale*deriv
        }
        fit <- list(beta = beta, iteration = iter, conv = (iter < max.iter))
        return(fit)
}
        
est.exp(0.15)
```

###### fourth fix
problem:
1. Problem: Locked into using specific columns of gmp { if we want touse a di erent data set, have to rewrite the function.
```{r}
est.exp <- function(beta, beta_0 = 6611,response = gmp$pcgmp,predictor = gmp$pop, max.iter = 100,stop.deriv = .01, deriv.step = .001,step.scale = 1e-12) {

        iter <- 0 # iteration counter
        deriv <- Inf

        ## add a function
        mse <- function(b) {mean((response - beta_0*predictor^b)^2)}

        
        while((iter < max.iter) & (deriv > stop.deriv)) {
        # the function will stop if the iteration is too long or the deriv is samller than the b
        iter <- iter + 1
        # calculate the differente
        deriv <- (mse(beta + deriv.step) - mse(beta))/deriv.step
        # update the value of beta
        beta <- beta - step.scale*deriv
        }
        fit <- list(beta = beta, iteration = iter, conv = (iter < max.iter))
        return(fit)
}
        
est.exp(0.15)
```

###### fifth fix
problem:
1. Want to make it easy for humans to read.
```{r}
est.exp <- function(beta, beta_0 = 6611, response = gmp$pcgmp,predictor = gmp$pop, max.iter = 100,stop.deriv = .01, deriv.step = .001,step.scale = 1e-12) {
        iter <- 0
        deriv <- Inf
        
        mse <- function(b) {mean((response - beta_0*predictor^b)^2)}

        for (i in 1:max.iter) {
        iter <- iter + 1
        deriv <- (mse(beta + deriv.step) - mse(beta))/deriv.step
        beta <- beta - step.scale*deriv
        if (abs(deriv) < stop.deriv) {break()}
}
        fit <- list(beta = beta, iteration = iter, conv = (iter < max.iter))
        return(fit)
}
est.exp(0.15)
```

##### classifiation
```{r}
library(ISLR)
head(Smarket,3)

mean(Smarket$Lag1[Smarket$Direction=='Up'])
mean(Smarket$Lag1[Smarket$Direction=='Down'])


plot(Smarket$Lag1, Smarket$Lag2, col = Smarket$Direction,xlab="Lag1", ylab="Lag2", main="Today's Direction")

# use lag1 and lag2 as example and draw a picture
legend("bottomright", legend = levels(Smarket$Direction),col=1:length(levels(Smarket$Direction)), pch=1)


KNNclass <- function(L1.new,L2.new,k=5, L1 = train$Lag1, L2 = train$Lag2, Dir = train$Direction){
        # k = 5 . new point (2,4.25)
        dists <- sqrt((L1-L1.new)^2 +(L2 -L2.new)^2)
        # pick up the nearest k point
        neighbors <- order(dists)[1:K]
        neigh.dir <- Dir[neighbors]
        choice <- names(which.max(table(neigh.dir)))
        choice
}

```




