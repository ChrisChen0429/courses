---
title: "lecture six"
author: "Yi (Chris) Chen"
date: "October 13, 2017"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### lecture six
#### cross-validation     
determine which model you should use when have no test data set: model selection

* some potential problems:
1. overfitting: too closely to specific properities of the training data, rather than the underlying distribution.    
  In KNN when k tends to be smaller, the error in training data tends to be smaller. (when k=1, the error in training data is 0 ) But this may tends to be larger error in testing data.

2. unadequate fitting

```{r}
library(ISLR)
kNNclass <- function(NewPoint,k=5,lags=Weekly[,2:6],Dir=Weekly$Direction){
        n <- nrow(lags)
        
        stopifnot(length(NewPoint)==5,ncol(lags)==5,k<=n)
        
        dists <- rowSums((lags - rep(NewPoint,each=n))^2)
        neighbours <- order(dists)[1:k]
        neighb.dir <- Dir[neighbours]
        choice <- names(which.max(table(neighb.dir)))
        
        return(choice)
}

Newpoint <- c(-.5,.5,-.5,-.5,.5)
kNNclass(Newpoint)
```

```{r}
test <- Weekly[Weekly$Year>=2009,]
train <- Weekly[Weekly$Year<2009,]

n.test <- nrow(test)
prediction <- rep(NA,n.test)

for(i in 1:n.test){
        newp          <- as.numeric(test[i,2:6])
        prediction[i] <- kNNclass(newp,lags = train[,2:6],Dir = train$Direction)
}

test.error <- mean(prediction != test$Direction)
test.error
```

```{r}
# divide the data into k=9 folds
k    <- 9
nums <- rep(1:k,each=nrow(Weekly)/k)
fold <- sample(nums)
```

```{r}
# iterate over the fold, getting an estimate of the test error
fold.error <- rep(NA, k)
for(j in 1:k){
        test  <- Weekly[fold==j,] 
        train <- Weekly[fold!=j,]
        
        n.test     <- nrow(test)
        prediction <- rep(NA,n.test)
        
        for(i in 1:n.test){
                newp          <- as.numeric(test[i,2:6])
                prediction[i] <- kNNclass(newp,lags = train[,2:6],Dir = train$Direction)
        }
        fold.error[j] <- mean(prediction != test$Direction)
}

approx.error <- mean(fold.error)
approx.error
```


```{r}
# iterate over parameter k, getting an estimate of the test error for each
K            <- seq(1,55,by=2)
approx.error <- rep(NA,length(K))
count        <- 1

for (kay in K) {
        fold.error <- rep(NA, k)
        for (j in 1:k) {
                test        <- Weekly[fold == j, ]
                train       <- Weekly[fold != j, ]
                n.test      <- nrow(test)
                predictions <- rep(NA, n.test)
        
                for (i in 1:n.test){
                        newp  <- as.numeric(test[i, 2:6])
                        out   <- kNNclass(newp, k= kay, lags = train[, 2:6],Dir = train$Direction)
                        predictions[i] <- out
                }
                fold.error[j]  <- mean(predictions != test$Direction)
        }
        approx.error[count]    <- mean(fold.error)
        count                  <- count + 1
}
```

```{r}
approx.error[1:4]
plot(K, approx.error, xlab = "K", lty = 1,ylab = "Approximate Test Error",main = "Predicting Market Direction", pch = 1)
lines(K, approx.error, lty = 1)
```


#### Plotting
```{r}
setwd("C:/Users/cheny/Desktop/study/statistical computing and intro to data science/lecture/lecture six")

diamonds <- read.csv("diamonds.csv", as.is = T)
lev_vec <- c("Fair", "Good", "Very Good", "Premium", "Ideal")
diamonds$cut <- factor(diamonds$cut, level = lev_vec)
diamonds$color <- factor(diamonds$color)
diamonds$clarity <- factor(diamonds$clarity)

set.seed(1)
rows <- dim(diamonds)[1]
diam <- diamonds[sample(1:rows, 1000), ]

plot(log(diam$carat),log(diam$price),col=diam$cut,pch=16)
legend('bottomright',legend = levels(diam$cut),fill = 1:length(diam$cut))
abline(0,8,col='orange',lty=2)
```


```{r}
plot(log(diam$carat),log(diam$price),col=diam$cut,pch=16)
legend('bottomright',legend = levels(diam$cut),fill = 1:length(diam$cut))
cuts <- levels(diam$cut)
col_counter <- 1
for (i in cuts) {
        this_cut <- diam$cut == i
        this_data <- diam[this_cut, ]
        this_lm <- lm(log(this_data$price) ~ log(this_data$carat))
        abline(this_lm, col = col_counter)
        col_counter <- col_counter + 1
}


points(-0.4,6.8,pc='*',col='purple',cex=1.5)
```

##### ggplot
```{r}
dim(mpg)

ggplot(data=mpg) + geom_point(mapping = aes(x=displ,y=hwy))
ggplot(data=mpg) + geom_point(mapping = aes(x=cyl,y=hwy))

ggplot(data=mpg) + geom_point(mapping = aes(x=displ,y=hwy,color = class))

ggplot(data=mpg) + geom_point(mapping = aes(x=displ,y=hwy,color = year))
                              
ggplot(data=mpg) + geom_point(mapping = aes(x=displ,y=hwy,size = cty))

ggplot(data=mpg) + geom_point(mapping = aes(x=displ,y=hwy,alpha = class))

ggplot(data=mpg) + geom_point(mapping = aes(x=displ,y=hwy,shape = class))

ggplot(data=mpg) + geom_point(mapping = aes(x=displ,y=hwy),color='blue')

ggplot(data=mpg) + geom_point(mapping = aes(x=displ,y=hwy)) + facet_wrap(~class,nrow = 2)

ggplot(data=mpg) + geom_point(mapping = aes(x=displ,y=hwy)) + facet_grid(drv~class)

ggplot(data=mpg) + geom_smooth(mapping = aes(x=displ,y=hwy))

ggplot(data=mpg) + geom_point(mapping = aes(x=displ,y=hwy)) + geom_smooth(mapping = aes(x=displ,y=hwy))


ggplot(data=mpg) + 
        geom_point(mapping = aes(x=displ,y=hwy))+
        geom_smooth(mapping = aes(x=displ,y=hwy))+
        geom_point(mapping = aes(x=3,y=30),color='purple')+
        geom_text(mapping = aes(x=3,y=30),label = 'new plot',size=4)+
        labs(title='new plot',x='english weight',y='highway mpg')


ggplot(data=diamonds,)

```

